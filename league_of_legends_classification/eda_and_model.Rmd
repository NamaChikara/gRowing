---
title: "LoL: Predict Game Outcome"
author: "ZackBarry"
date: "4/22/2020"
output: 
  html_document:
   number_sections: true
   toc: true
   fig_width: 7
   fig_height: 4.5
   theme: cosmo
   highlight: tango
   code_folding: show
---

```{r setup, message = FALSE, warning = FALSE }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DataExplorer)
library(ggpubr)
library(leaflet)
library(leaflet.extras)
library(sqldf)
library(patchwork)
library(caret)
library(knitr)
library(DT)
library(summarytools)
library(purrr)
library(car) # for vif()

dataset <- read_csv("Data/high_diamond_ranked_10min.csv")
```

# Introduction

Leage of Legends is one of the most popular online multiplier games.  Two teams of 5 players compete to battle their way to their oponents' base.  From game to game, players can assume different characters and roles on their team.

The goal of this notebook is to predict the outcome of a game with data from the first 10 minutes.  Typically, games last 35min-45min, so it will be interesting to see how telling the first 10 minutes are.  The dataset contains 19 different KPIs per team across 10,000 games.  

# Pre-Modeling Stages

## Acquiring/Loading Data

We can see that the same variables are available for each the "red" and "blue" team, except `blueWins` records the outcome (there is no `redWins`).
```{r}
dataset %>% str()
```

No columns have missing data.
```{r}
map_df(dataset, function(x) { sum(is.na(x)) }) 
```

## Data Wrangling: 1 row per team

We'd like to be able to see distributions of the winning teams' KPIs alongside the losing teams' KPIs.  Currently, the losing and winning team for each map occupy the same row.  Here we modify the data set so that each row is one team's performance in a given game:
```{r}
blue_df <- dataset %>%
  select_at(vars(-contains("red")))

red_df <- dataset %>%
  mutate(redWins = 1 - blueWins) %>%
  select_at(vars(-contains("blue")))

colnames(blue_df) <- str_replace(colnames(blue_df), "blue([a-zA-Z]*)", "\\1")
colnames(red_df) <- str_replace(colnames(red_df), "red([a-zA-Z]*)", "\\1")

long_df <- rbind(blue_df, red_df)
```

There are no values that we need to impute in this dataset.

## Visualization

### Variable Correlation

Next, we look at a correlation heat map with all variables.  There are too many variables to get a clear sense of what's going on, so we'll break it down in the next couple visualizations.
```{r}
cor_mat <- cor(long_df)
# Melt the correlation matrix
library(reshape2)
melted_cormat <- melt(cor_mat, na.rm = TRUE) 
upper_tri <- melted_cormat %>%
  mutate(Var1 = as.character(Var1), Var2 = as.character(Var2)) %>%
  filter(Var1 < Var2) %>%
  mutate(Var1 = as.factor(Var1), Var2 = as.factor(Var2))
# Heatmap
ggplot(data = upper_tri, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

We consider which variables are highly correlated (cor > 0.5).  We see that `TotalGold` and `TotalExperience` are the variables with the most highly correlated pairs.  This makes sense because they are high-level metrics that are likely influenced by lower level metrics such as `AvgLevel` and `GoldDiff`.
```{r}
upper_tri %>%
  filter(abs(value) > 0.5) %>%
  ggplot(aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

Next, we look to see if any variables are 100\% correlated.  We find that `GoldPerMin` is 100\% correlated with `TotalGold` and `CSPerMin` with `TotalMinionsKilled`.  As such we will drop `TotalGold` and `TotalMinionsKilled` when we prepare for modeling.
```{r}
filter(upper_tri, value == 1)
```

### KPI Performance of Winning vs. Losing Team

Our first in-depth look will be the correlation of each variable with Wins.  We find that gold-related KPIs `GoldDiff`, `ExperienceDiff`, `TotalGold`, and `GoldPerMin` top the list.  Experience KPIs `TotalExperience` and `AvgLevel` are slightly less correlated, as are player vs. player aggression KPIs `Kills`, `Deaths`, and `Assists`. Monster, Minion, and Ward KPIs are the least correlated.  It will be interesting to see if our Primary Component Analysis pulls these groups out.
```{r}
as_tibble(cor_mat) %>%
  mutate(comparedVar = rownames(cor_mat)) %>%
  select(comparedVar, Wins) %>%
  arrange(desc(abs(Wins)))
``` 

Taking a look at the distributions of the top 5 variables that are most correlated with `Wins`, we see that each distribution is approximately normal.  Additionally, the distribution for the variable when `Wins` is 0 is generally shifted to the left from when `Wins` is 1.  This is in line with out expectations - it makes sense that the losing team should have less Gold and Experience than the winning team.
```{r}
top_5_most_correlated <- c("GoldDiff", "ExperienceDiff", "TotalGold", "GoldPerMin", "TotalExperience")

compare_histogram <- function(df, kpi_name) {
  
  wrapr::let(
    alias = list(KPI_VAR = kpi_name),
    expr = {
      ggplot(df, aes(x = KPI_VAR, color = factor(Wins))) +
        geom_histogram(position = "identity", alpha = 0.5, fill = "white") +
        theme_bw()
    }
  )

}

plots <- lapply(top_5_most_correlated, compare_histogram, df = long_df) 
wrap_plots(plots)
```

### VIF: Another Look at Correlation

```{r}
long_df <- select(long_df, -c(TotalGold, TotalMinionsKilled))

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

```{r}
alias(vif_model)
```

```{r}
long_df <- select(long_df, -EliteMonsters)

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

### PCA

Next we seek to reduce the dimension of our predictor space by applying Principal Component Analysis (PCA).  PCA uses linear combinations of possibly correlated input variables to form a new set of variables (principal components) that are uncorrelated with one another.  Additionally, the variables are created sequentially to explain the most variance possible in the dataset.  A variance threshold can be used for dimensionality reduction (e.g. keep only those components that explain more than 5\% of the variance). Note: we center and scale (standardize) the data before applying PCA since variables with larger mean or standard deviation will be prioritized to explain the variation of the data.  

In the scree plot below, the magnitude of the eigenvalue indicates the amount of variation that each principal component captures.  The proportion of variance for a given component is the component's eigenvalue divided by the sum of all eigenvalues.  We see that the first component  and second components explain ~35\%  and ~13\% of the variance respectively. Later components are similar in the amount of variance they explain.  The bottom plot shows the cumulative variance explained by the first N components.  One rule of thumb is to keep enough components so that this cumulative variance exceeds 80\%.  In this case, 7 variables appears to be sufficient.
```{r}
pca_results <- prcomp(~ . - gameId - Wins, data = long_df, center = TRUE, scale = TRUE)

# scree plot
var_proportion <- pca_results %>%
  summary() %>%
  `[[`("importance") %>%
  t() %>%
  as.data.frame() %>%
  mutate(component_number = str_replace(rownames(.), "PC", "")) %>%
  mutate(component_number = as.numeric(component_number)) %>%
  rename_all(~tolower(str_replace_all(., "\\s+", "_"))) %>%
  mutate(eigenvalue = standard_deviation^2)

scree_plot <- ggplot(var_proportion, aes(x = component_number, y = eigenvalue)) +
  geom_col() 

prop_plot <- ggplot(var_proportion, aes(x = component_number, y = proportion_of_variance)) +
  geom_col()

cum_prop_plot <- ggplot(var_proportion, aes(x = component_number, y = cumulative_proportion)) +
  geom_line() +
  geom_hline(yintercept = 0.8)

(scree_plot + prop_plot) / cum_prop_plot
```

Next we consider a loading plot which shows how strongly each input variable influences a principal component.  The length of the vector along the PCX axis (i.e. the length of the vector projected to the PCX axis) indicates how much weight that variable has on PCX.  The angles between vectors tell us how the variables are correlated with one another.  If two vectors are close, the variables they represent are positively correlated.  If the angle is closer to 90 degrees, they are not likely to be correlated.  If the angle is close to 180 degress they are negatively correlated.

In this example, the variables that contribute most to `PC1` are `GoldDiff`, `ExperienceDiff`, `AvgLevel` and `TotalExperience`. Not many variables contribute more to `PC2` than to `PC1`, but `Kills`, `Assists`, and `Deaths` contribute significantly.  `PC1` appears to represent high-level team characteristics, while `PC2` represents the biggest KPIs within a game.

We see that `Kills` and `Assists` are strongly correlated with one another.  `Deaths` is negatively correlated with `AvgLevel` and `TotalExperience`.
```{r}
pca_loadings <- data.frame(variable = rownames(pca_results$rotation), pca_results$rotation)

get_loadings_plot <- function(loadings_df, pc_a, pc_b) {
  wrapr::let(
    alias = list(PCOMP_A = pc_a, PCOMP_B = pc_b),
    expr = {
      ggplot(loadings_df) +
        geom_segment(
          aes(x = 0, y = 0, xend = PCOMP_A, yend = PCOMP_B), 
          arrow = arrow(length = unit(1/2, "picas")),
          color = "black"
        ) +
        annotate(
          "text", 
          x = loadings_df$PCOMP_A*1.2, y = loadings_df$PCOMP_B*1.2,
          label = loadings_df$variable
        ) +
        labs(x = pc_a, y = pc_b) +
        theme_bw()
    }
  )
}

get_loadings_plot(pca_loadings, "PC1", "PC2")
```

Finally, we visualize the contribution of each variable to each component with a heat map.  To make it easier to identify contributing variables, those with loading values less than 0.2 are colored as white.
```{r}
pca_loadings %>%
  melt(variable.name = "component") %>% 
  ggplot(aes(x = component, y = variable, fill = abs(value))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "white", high = "green", mid = "white",
        midpoint = 0.2, limit = c(0, 1), space = "Lab",
        name = "Loading Value"
    ) +
    theme_minimal() +
    coord_fixed()
```

## Data Wrangling: 1 row per game

```{r}
comparable_cols <- colnames(dataset)[str_detect(colnames(dataset), "red")]
comparable_cols <- str_replace(comparable_cols, "red", "")
comparable_cols <- setdiff(comparable_cols, c("ExperienceDiff", "GoldDiff"))

narrow_df <- dataset

for (col in comparable_cols) {
  
  wrapr::let(
    alias = list(B_VAR = paste0("blue", col), R_VAR = paste0("red", col), D_VAR = paste0("percdiff", col)),
    expr = {
      narrow_df <- narrow_df %>%
        mutate(D_VAR = ifelse(B_VAR == 0 & R_VAR == 0, 0,
                              100 * 2 * (B_VAR - R_VAR) / (B_VAR + R_VAR))
        )
    }
  )
  
}

narrow_df <- cbind(
    select(narrow_df, "gameId", "blueWins"),
    select_at(narrow_df, vars(contains("percdiff")))
  )
```

## Visualization

### Variable Correlation

Next, we look at a correlation heat map with all variables.  There are too many variables to get a clear sense of what's going on, so we'll break it down in the next couple visualizations.
```{r}
cor_mat <- cor(narrow_df)
melted_cormat <- melt(cor_mat, na.rm = TRUE) 
upper_tri <- melted_cormat %>%
  mutate(Var1 = as.character(Var1), Var2 = as.character(Var2)) %>%
  filter(Var1 < Var2) %>%
  mutate(Var1 = as.factor(Var1), Var2 = as.factor(Var2))
# Heatmap
ggplot(data = upper_tri, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

We consider which variables are highly correlated (cor > 0.5).  We see that `TotalGold` and `TotalExperience` are the variables with the most highly correlated pairs.  This makes sense because they are high-level metrics that are likely influenced by lower level metrics such as `AvgLevel` and `GoldDiff`.
```{r}
upper_tri %>%
  filter(abs(value) > 0.5) %>%
  ggplot(aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

Next, we look to see if any variables are 100\% correlated.  We find that `GoldPerMin` is 100\% correlated with `TotalGold` and `CSPerMin` with `TotalMinionsKilled`.  As such we will drop `TotalGold` and `TotalMinionsKilled` when we prepare for modeling.
```{r}
filter(upper_tri, value == 1)
```

### KPI Performance of Winning vs. Losing Team

Our first in-depth look will be the correlation of each variable with Wins.  We find that gold-related KPIs `GoldDiff`, `ExperienceDiff`, `TotalGold`, and `GoldPerMin` top the list.  Experience KPIs `TotalExperience` and `AvgLevel` are slightly less correlated, as are player vs. player aggression KPIs `Kills`, `Deaths`, and `Assists`. Monster, Minion, and Ward KPIs are the least correlated.  It will be interesting to see if our Primary Component Analysis pulls these groups out.
```{r}
as_tibble(cor_mat) %>%
  mutate(comparedVar = rownames(cor_mat)) %>%
  select(comparedVar, blueWins) %>%
  arrange(desc(abs(blueWins)))
``` 

Taking a look at the distributions of the top 5 variables that are most correlated with `Wins`, we see that each distribution is approximately normal.  Additionally, the distribution for the variable when `Wins` is 0 is generally shifted to the left from when `Wins` is 1.  This is in line with out expectations - it makes sense that the losing team should have less Gold and Experience than the winning team.
```{r}
top_5_most_correlated <- c("percdiffGoldPerMin", "percdiffTotalGold", "percdiffTotalExperience", "percdiffKills", "percdiffDeaths")

compare_histogram <- function(df, kpi_name) {
  
  wrapr::let(
    alias = list(KPI_VAR = kpi_name),
    expr = {
      ggplot(df, aes(x = KPI_VAR, color = factor(blueWins))) +
        geom_histogram(position = "identity", alpha = 0.5, fill = "white") +
        theme_bw()
    }
  )

}

plots <- lapply(top_5_most_correlated, compare_histogram, df = narrow_df) 
wrap_plots(plots)
```

### PCA

```{r}
pca_results <- prcomp(~ . - gameId - blueWins, data = narrow_df, center = TRUE, scale = TRUE)

# scree plot
var_proportion <- pca_results %>%
  summary() %>%
  `[[`("importance") %>%
  t() %>%
  as.data.frame() %>%
  mutate(component_number = str_replace(rownames(.), "PC", "")) %>%
  mutate(component_number = as.numeric(component_number)) %>%
  rename_all(~tolower(str_replace_all(., "\\s+", "_"))) 

scree_plot <- ggplot(var_proportion, aes(x = component_number, y = proportion_of_variance)) +
  geom_col() 

cum_prop_plot <- ggplot(var_proportion, aes(x = component_number, y = cumulative_proportion)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.9)

scree_plot + cum_prop_plot
```


```{r}
pca_loadings <- data.frame(variable = rownames(pca_results$rotation), pca_results$rotation)

get_loadings_plot <- function(loadings_df, pc_a, pc_b) {
  wrapr::let(
    alias = list(PCOMP_A = pc_a, PCOMP_B = pc_b),
    expr = {
      ggplot(loadings_df) +
        geom_segment(
          aes(x = 0, y = 0, xend = PCOMP_A, yend = PCOMP_B), 
          arrow = arrow(length = unit(1/2, "picas")),
          color = "black"
        ) +
        annotate(
          "text", 
          x = loadings_df$PCOMP_A*1.2, y = loadings_df$PCOMP_B*1.2,
          label = loadings_df$variable
        ) +
        labs(x = pc_a, y = pc_b)
    }
  )
}

get_loadings_plot(pca_loadings, "PC1", "PC2")
```

```{r}
pca_loadings %>%
  melt(variable.name = "component") %>% 
  ggplot(aes(x = component, y = variable, fill = abs(value))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "white", high = "green", mid = "white",
        midpoint = 0.2, limit = c(0, 1), space = "Lab",
        name = "Loading Value"
    ) +
    theme_minimal() +
    coord_fixed()
```

## Data Wrangling: 1 row per game

## Visualization

### Variable Correlation

Next, we look at a correlation heat map with all variables.  There are too many variables to get a clear sense of what's going on, so we'll break it down in the next couple visualizations.
```{r}
cor_mat <- cor(dataset)
melted_cormat <- melt(cor_mat, na.rm = TRUE) 
upper_tri <- melted_cormat %>%
  mutate(Var1 = as.character(Var1), Var2 = as.character(Var2)) %>%
  filter(Var1 < Var2) %>%
  mutate(Var1 = as.factor(Var1), Var2 = as.factor(Var2))
# Heatmap
ggplot(data = upper_tri, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

We consider which variables are highly correlated (cor > 0.5).  We see that `TotalGold` and `TotalExperience` are the variables with the most highly correlated pairs.  This makes sense because they are high-level metrics that are likely influenced by lower level metrics such as `AvgLevel` and `GoldDiff`.
```{r}
upper_tri %>%
  filter(abs(value) > 0.5) %>%
  ggplot(aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

Next, we look to see if any variables are 100\% correlated.  We find that `GoldPerMin` is 100\% correlated with `TotalGold` and `CSPerMin` with `TotalMinionsKilled`.  As such we will drop `TotalGold` and `TotalMinionsKilled` when we prepare for modeling.
```{r}
filter(upper_tri, value == 1)
```

### KPI Performance of Winning vs. Losing Team

Our first in-depth look will be the correlation of each variable with Wins.  We find that gold-related KPIs `GoldDiff`, `ExperienceDiff`, `TotalGold`, and `GoldPerMin` top the list.  Experience KPIs `TotalExperience` and `AvgLevel` are slightly less correlated, as are player vs. player aggression KPIs `Kills`, `Deaths`, and `Assists`. Monster, Minion, and Ward KPIs are the least correlated.  It will be interesting to see if our Primary Component Analysis pulls these groups out.
```{r}
as_tibble(cor_mat) %>%
  mutate(comparedVar = rownames(cor_mat)) %>%
  select(comparedVar, blueWins) %>%
  arrange(desc(abs(blueWins)))
``` 

Taking a look at the distributions of the top 5 variables that are most correlated with `Wins`, we see that each distribution is approximately normal.  Additionally, the distribution for the variable when `Wins` is 0 is generally shifted to the left from when `Wins` is 1.  This is in line with out expectations - it makes sense that the losing team should have less Gold and Experience than the winning team.
```{r}
top_5_most_correlated <- c("percdiffGoldPerMin", "percdiffTotalGold", "percdiffTotalExperience", "percdiffKills", "percdiffDeaths")

compare_histogram <- function(df, kpi_name) {
  
  wrapr::let(
    alias = list(KPI_VAR = kpi_name),
    expr = {
      ggplot(df, aes(x = KPI_VAR, color = factor(blueWins))) +
        geom_histogram(position = "identity", alpha = 0.5, fill = "white") +
        theme_bw()
    }
  )

}

plots <- lapply(top_5_most_correlated, compare_histogram, df = dataset) 
wrap_plots(plots)
```

### PCA

```{r}
pca_results <- prcomp(~ . - gameId - blueWins, data = dataset, center = TRUE, scale = TRUE)

# scree plot
var_proportion <- pca_results %>%
  summary() %>%
  `[[`("importance") %>%
  t() %>%
  as.data.frame() %>%
  mutate(component_number = str_replace(rownames(.), "PC", "")) %>%
  mutate(component_number = as.numeric(component_number)) %>%
  rename_all(~tolower(str_replace_all(., "\\s+", "_"))) 

scree_plot <- ggplot(var_proportion, aes(x = component_number, y = proportion_of_variance)) +
  geom_col() 

cum_prop_plot <- ggplot(var_proportion, aes(x = component_number, y = cumulative_proportion)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.9)

scree_plot + cum_prop_plot
```


```{r}
pca_loadings <- data.frame(variable = rownames(pca_results$rotation), pca_results$rotation)

get_loadings_plot <- function(loadings_df, pc_a, pc_b) {
  wrapr::let(
    alias = list(PCOMP_A = pc_a, PCOMP_B = pc_b),
    expr = {
      ggplot(loadings_df) +
        geom_segment(
          aes(x = 0, y = 0, xend = PCOMP_A, yend = PCOMP_B), 
          arrow = arrow(length = unit(1/2, "picas")),
          color = "black"
        ) +
        annotate(
          "text", 
          x = loadings_df$PCOMP_A*1.2, y = loadings_df$PCOMP_B*1.2,
          label = loadings_df$variable
        ) +
        labs(x = pc_a, y = pc_b)
    }
  )
}

get_loadings_plot(pca_loadings, "PC1", "PC2")
```

```{r}
pca_loadings %>%
  melt(variable.name = "component") %>% 
  ggplot(aes(x = component, y = variable, fill = abs(value))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "white", high = "green", mid = "white",
        midpoint = 0.2, limit = c(0, 1), space = "Lab",
        name = "Loading Value"
    ) +
    theme_minimal() +
    coord_fixed()
```


## Model fitting

```{r}
# mlr3 classification task needs target to be a factor
dataset <- mutate(dataset, blueWins = as.factor(blueWins))

train <- sample_frac(dataset, 0.9)
test <- anti_join(dataset, train)
```

### mlr3 Walkthrough: Decision Tree

To determine a baseline for model performance, we first estimate the classification accuracy for a decision tree by using cross validation.  This also gives us the opportunity to walk through the ml3 modeling process in detail.  In mlr3 applying cross-validation requires three objects - (1) a `learner` containing the model to be trained; (2) a `task` containing the data to be resampled from; (3) a `resampling` that defines the sampling method.  

#### Task

A Task wraps a DataBackend which provides a layer of abstraction for various data storage systems (e.g. DataFrames).  mlr3 comes with a data.table implementation for backends; the conversion from DataFrame to data.table is done automatically.  Instead of working directly with DataBackends, they are worked with indirectly through the Task they are associated with.  Tasks also store information about the role of individual columns of the DataBackend (e.g. target vs. feature).
```{r}
task = TaskClassif$new("wins_classifier", backend = train, target = "blueWins", positive = "1")
task
```

Deselect linearly dependent variables determined by EDA above - this is done in-place and "provides a different 'view' on the data without altering the data itself".
```{r}
task$select(cols = setdiff(task$feature_names, c("blueTotalGold", "blueTotalMinionsKilled", "redKills", "redTotalGold", "redTotalMinionsKilled")))
```

#### Resampling

See the different resampling implementations:
```{r}
as.data.table(mlr_resamplings)  %>%
  unnest_wider(params, names_sep = "")
```

The verbose way to do this is to retrieve the specific Resampling object, set the hyperparameters, and attach it to a task in 3 separate steps:
```{r}
my_cv = mlr_resamplings$get("cv")
my_cv$param_set$values <- list(folds = 5)
my_cv$instantiate(task)
my_cv
```
An easier way is to use `rsmp` to retrieve the object and set hyperparameters in one go:
```{r}
my_cv <- rsmp("cv", folds = 5)$instantiate(task)
my_cv
```

#### Learner

See the different built-in learners:
```{r}
as.data.table(mlr_learners)  %>%
  filter(str_detect(key, "classif"))
```

Attach additional learners with `mlr3learners` package:
```{r}
library(mlr3learners)
as.data.table(mlr_learners)  %>%
  filter(str_detect(key, "classif"))
```

As with resampling methods, there is a verbose way and a concise way to get a learner:
```{r}
my_learner = mlr_learners$get("classif.rpart")
my_learner
```

The concise way:
```{r}
my_learner = lrn("classif.rpart")
my_learner
```

#### Apply Cross Validation

A call to `resample` returnes a `ResampleResult` object that can be used to access different models and metrics from the CV runs.
```{r}
my_resample <- resample(task = task, learner = my_learner, resampling = my_cv, store_models = TRUE)
my_resample
```

#### Retrieve Performance Metrics

We can aggregate model-specific metrics user `Measure` objects. Here is a list of different built-in measures:
```{r}
as.data.table(mlr_measures) %>%
  filter(task_type == "classif")
```

To calculate a given measure for each CV model, we first create a measure:
```{r}
my_measure <- mlr_measures$get("classif.acc")
my_measure
```

And next we pass the measure to the `score()` method attached to our resampling object:
```{r}
my_resample$score(my_measure) %>%
  select(iteration, classif.acc)
```

We can also do this in a more concise way using the `msr()` function:
```{r}
my_resample$score(msr("classif.acc")) %>%
  select(iteration, classif.acc)
```

Instead of using `score()`, we can use `aggregate()` to get a summary statistic across all models:
```{r}
my_resample$aggregate(msr("classif.acc"))
```

And we can also pass multiple statistics:
```{r}
my_resample$aggregate(msrs(c("classif.acc", "classif.recall")))
```

### mlr3


```{r}
taskTrain = TaskClassif$new("wins_classifier", backend = train, target = "blueWins", positive = "1")

taskTrain$select(cols = setdiff(taskTrain$feature_names, c("blueTotalGold", "blueTotalMinionsKilled", "redKills", "redTotalGold", "redTotalMinionsKilled")))
```

```{r}
PipeOpPrepend = R6::R6Class("PipeOpPrepend",
  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "prepend", param_vals = list()) {
      ps = ParamSet$new(params = list(ParamUty$new("prefix", default = "", tags = "prefix")))
      super$initialize(id, param_set = ps, param_vals = param_vals)
    },
    
    get_state = function(task) {
      old_names = task$feature_names
      new_names = paste0(self$param_set$get_values(tags = "prefix"), old_names)
      list(old_names = old_names, new_names = new_names)
    },

    transform = function(task) {
      task$rename(self$state$old_names, self$state$new_names)
    }
  )
)
```


```{r}
blue_cols <- po("select", id = "blue_cols")
blue_cols$param_set$values$selector <- selector_grep("blue")

blue_pca <- po("pca", id = "blue_pca")
blue_pca$param_set$values <- list(center = TRUE, scale. = TRUE)

blue_pca_entire <- po("select", id = "blue_cols", param_vals = list(selector = selector_grep("blue"))) %>>%
  po("pca", id = "blue_pca", param_vals = list(center = TRUE, scale. = TRUE))


```

```{r}
pca_entire <- po("select", id = "blue_cols", param_vals = list(selector = selector_grep("blue"))) %>>%
  po("pca", id = "blue_pca", param_vals = list(center = TRUE, scale. = TRUE)) %>>%
  PipeOpPrepend$new(id = "blue_pca_rename", param_vals = list(prefix = "blue_")) %>>%
  po("select", id = "red_cols", param_vals = list(selector = selector_grep("red"))) %>>%
  po("pca", id = "red_pca", param_vals = list(center = TRUE, scale. = TRUE)) %>>%
  PipeOpPrepend$new(id = "red_pca_rename", param_vals = list(prefix = "red_"))
```


```{r}
pca_blue <- po("select", id = "blue_cols", param_vals = list(selector = selector_grep("blue"))) %>>%
  po("pca", id = "blue_pca", param_vals = list(center = TRUE, scale. = TRUE)) %>>%
  PipeOpPrepend$new(id = "blue_pca_rename", param_vals = list(prefix = "blue_"))

pca_red <- po("select", id = "red_cols", param_vals = list(selector = selector_grep("red"))) %>>%
  po("pca", id = "red_pca", param_vals = list(center = TRUE, scale. = TRUE))  %>>%
  PipeOpPrepend$new(id = "red_pca_rename", param_vals = list(prefix = "red_"))

graph <- gunion(list(pca_blue, pca_red)) %>>%
  po("featureunion") %>>%
  PipeOpLearner$new(lrn("classif.rpart", predict_type = "response"))

graph$keep_results <- TRUE

graph$plot(html = TRUE)
```

```{r}
graph_learner = GraphLearner$new(graph)

cv5 = rsmp("cv", folds = 5)
result <- resample(taskTrain, graph_learner, cv5)

result$aggregate(msrs(c("classif.acc", "classif.recall", "classif.sensitivity")))
```

```{r}
library(mlr3tuning)

tune_ps = ParamSet$new(list(
  ParamInt$new("blue_pca.rank.", lower = 1, upper = 12),
  ParamInt$new("red_pca.rank.", lower = 1, upper = 12)
))

tune_ps = ParamSet$new(list(
  ParamInt$new("classif.rpart.minbucket", lower = 1, upper = 12)
))

term_spec = term("evals", n_evals = 20)

instance = TuningInstance$new(
  task = taskTrain,
  learner = graph_learner,
  resampling = cv5,
  measures = msr("classif.acc"),
  param_set = tune_ps,
  terminator = term_spec
)

instance
```


```{r}
tuner = tnr("grid_search", resolution = 10)

result = tuner$tune(instance)
```


```{r}
library(mlr3)
library(mlr3pipelines)
library(paradox)
library(dplyr)

PipeOpPrepend = R6::R6Class("PipeOpPrepend",
  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,
  public = list(
    initialize = function(id = "prepend", param_vals = list()) {
      ps = ParamSet$new(params = list(ParamUty$new("prefix", default = "", tags = "prefix")))
      super$initialize(id, param_set = ps, param_vals = param_vals)
    },
    
    get_state = function(task) {
      old_names = task$feature_names
      new_names = paste0(self$param_set$get_values(tags = "prefix"), old_names)
      list(old_names = old_names, new_names = new_names)
    },

    transform = function(task) {
      task$rename(self$state$old_names, self$state$new_names)
    }
  )
)

toy_df <- data.frame(
    homeWins = c(1, 0, 1, 0),
    homeHits = c(13,10,12,11),
    homeHomeRuns = c(0,2,0,0),
    awayHits = c(10,11,14,8),
    awayHomeRuns = c(1,0,1,1)
) %>%
  mutate(homeWins = as.factor(homeWins))

task = TaskClassif$new("toy_task", backend = toy_df, target = "homeWins", positive = "1")

pca_home = po("select", id = "home_cols", param_vals = list(selector = selector_grep("home"))) %>>%
  po("pca", id = "home_pca", param_vals = list(center = TRUE, scale. = TRUE)) %>>%
  PipeOpPrepend$new(id = "home_rename", param_vals = list(prefix = "home_"))

pca_away = po("select", id = "away_cols", param_vals = list(selector = selector_grep("away"))) %>>%
  po("pca", id = "away_pca", param_vals = list(center = TRUE, scale. = TRUE)) %>>%
  PipeOpPrepend$new(id = "away_rename", param_vals = list(prefix = "away_"))

graph <- gunion(list(pca_home, pca_away)) %>>%
  po("featureunion", id = "all_together") 

graph$train(task)
```


```{r}
PipeOpPrepend = R6::R6Class("PipeOpPrepend",
  inherit = mlr3pipelines::PipeOpTaskPreproc,
  public = list(
    initialize = function(id = "prepend", param_vals = list()) {
      ps = ParamSet$new(params = list(ParamUty$new("prefix", default = "", tags = "prefix")))
      super$initialize(id, param_set = ps, param_vals = param_vals)
    },

    train_task = function(task) {
      self$state = list()
      task$rename(task$feature_names, paste0(self$param_set$get_values(tags = "prefix"), task$feature_names))
    },

    predict_task = function(task) {
      task$rename(task$feature_names, paste0(self$param_set$get_values(tags = "prefix"), task$feature_names))
    }
  )
)
```
```{r}
# Sepal.Length = 35; Sepal.Width = 23; Petal.Length = 43; Petal.Width = 22
# 42L results in 44 bins

task = mlr_tasks$get("iris")
# 42 breaks --> 44 groups; each field has <= 43 distinct values
op = PipeOpHistBin$new(param_vals = list(breaks = 42L))
expect_pipeop(op)
result = op$train(list(task))
expect_task(result[[1L]])

d = apply(result[[1L]]$data(cols = result[[1L]]$feature_names), MARGIN = 2L,
          function(x) expect_lt(length(unique(x)), 44L))

op2 = PipeOpHistBin$new(param_vals = list(breaks = 7L))
result2 = op2$train(list(task))

e = mapply(function(x, y) expect_lte(length(unique(x)), length(unique(y))),
           x = result2[[1L]]$data(cols = result2[[1L]]$feature_names),
           y = result[[1L]]$data(cols = result[[1L]]$feature_names))

```
bins = sapply(result[[1L]]$data(), FUN = levels)[2:5]
```{r}
test_that("PipeOpHistBin - not all bins present", {
  task1 = mlr_tasks$get("iris")
  dat = iris
  dat$Sepal.Width[[1L]] = 2.13
  task2 = TaskClassif$new("iris2", backend = dat, target = "Species")
  
  op = PipeOpHistBin$new(param_vals = list(breaks = seq(0, 10, by = 0.05)))
  expect_pipeop(op)
  
  # task1 does not have a Sepal.Width value within the interval (2.10, 2.15]
  bin_to_check = cut(c(2.10, 2.2), 2)[1] # (2.10, 2.15]
  
  result1 = op$train(list(task1))
  expect_false(bin_to_check %in% result1[[1L]]$data()$Sepal.Width)
  
  result2 = op$predict(list(task2))
  expect_true(bin_to_check %in% result2[[1L]]$data()$Sepal.Width)
  
  result3 = op$train(list(task2))
  expect_equal(result2[[1L]]$data(), result3[[1L]]$data())
})
```














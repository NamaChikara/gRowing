---
title: "LoL: Predict Game Outcome"
author: "ZackBarry"
date: "4/22/2020"
output: 
  html_document:
   number_sections: true
   toc: true
   fig_width: 7
   fig_height: 4.5
   theme: cosmo
   highlight: tango
   code_folding: show
---

```{r setup, message = FALSE, warning = FALSE }
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DataExplorer)
library(ggpubr)
library(leaflet)
library(leaflet.extras)
library(sqldf)
library(patchwork)
library(caret)
library(knitr)
library(DT)
library(summarytools)
library(purrr)
library(car) # for vif()

dataset <- read_csv("Data/high_diamond_ranked_10min.csv")
```

# Introduction

Leage of Legends is one of the most popular online multiplier games.  Two teams of 5 players compete to battle their way to their oponents' base.  From game to game, players can assume different characters and roles on their team.

The goal of this notebook is to predict the outcome of a game with data from the first 10 minutes.  Typically, games last 35min-45min, so it will be interesting to see how telling the first 10 minutes are.  The dataset contains 19 different KPIs per team across 10,000 games.  

# Pre-Modeling Stages

## Acquiring/Loading Data

We can see that the same variables are available for each the "red" and "blue" team, except `blueWins` records the outcome (there is no `redWins`).
```{r}
dataset %>% head()
```

No columns have missing data.
```{r}
map_df(dataset, function(x) { sum(is.na(x)) }) 
```

## Data Wrangling: 1 row per team

We'd like to be able to see distributions of the winning teams' KPIs alongside the losing teams' KPIs.  Currently, the losing and winning team for each map occupy the same row.  Here we modify the data set so that each row is one team's performance in a given game:
```{r}
blue_df <- dataset %>%
  select_at(vars(-contains("red")))

red_df <- dataset %>%
  mutate(redWins = 1 - blueWins) %>%
  select_at(vars(-contains("blue")))

colnames(blue_df) <- str_replace(colnames(blue_df), "blue([a-zA-Z]*)", "\\1")
colnames(red_df) <- str_replace(colnames(red_df), "red([a-zA-Z]*)", "\\1")

long_df <- rbind(blue_df, red_df)
```

There are no values that we need to impute in this dataset.

## Visualization

### Variable Correlation

Next, we look at a correlation heat map with all variables.  There are too many variables to get a clear sense of what's going on, so we'll break it down in the next couple visualizations.
```{r}
cor_mat <- cor(long_df)
# Melt the correlation matrix
library(reshape2)
melted_cormat <- melt(cor_mat, na.rm = TRUE) 
upper_tri <- melted_cormat %>%
  mutate(Var1 = as.character(Var1), Var2 = as.character(Var2)) %>%
  filter(Var1 < Var2) %>%
  mutate(Var1 = as.factor(Var1), Var2 = as.factor(Var2))
# Heatmap
ggplot(data = upper_tri, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

We consider which variables are highly correlated (cor > 0.5).  We see that `TotalGold` and `TotalExperience` are the variables with the most highly correlated pairs.  This makes sense because they are high-level metrics that are likely influenced by lower level metrics such as `AvgLevel` and `GoldDiff`.
```{r}
upper_tri %>%
  filter(abs(value) > 0.5) %>%
  ggplot(aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

Next, we look to see if any variables are 100\% correlated.  We find that `GoldPerMin` is 100\% correlated with `TotalGold` and `CSPerMin` with `TotalMinionsKilled`.  As such we will drop `TotalGold` and `TotalMinionsKilled` when we prepare for modeling.
```{r}
filter(upper_tri, value == 1)
```

### KPI Performance of Winning vs. Losing Team

Our first in-depth look will be the correlation of each variable with Wins.  We find that gold-related KPIs `GoldDiff`, `ExperienceDiff`, `TotalGold`, and `GoldPerMin` top the list.  Experience KPIs `TotalExperience` and `AvgLevel` are slightly less correlated, as are player vs. player aggression KPIs `Kills`, `Deaths`, and `Assists`. Monster, Minion, and Ward KPIs are the least correlated.  It will be interesting to see if our Primary Component Analysis pulls these groups out.
```{r}
as_tibble(cor_mat) %>%
  mutate(comparedVar = rownames(cor_mat)) %>%
  select(comparedVar, Wins) %>%
  arrange(desc(abs(Wins)))
``` 

Taking a look at the distributions of the top 5 variables that are most correlated with `Wins`, we see that each distribution is approximately normal.  Additionally, the distribution for the variable when `Wins` is 0 is generally shifted to the left from when `Wins` is 1.  This is in line with out expectations - it makes sense that the losing team should have less Gold and Experience than the winning team.
```{r}
top_5_most_correlated <- c("GoldDiff", "ExperienceDiff", "TotalGold", "GoldPerMin", "TotalExperience")

compare_histogram <- function(df, kpi_name) {
  
  wrapr::let(
    alias = list(KPI_VAR = kpi_name),
    expr = {
      ggplot(df, aes(x = KPI_VAR, color = factor(Wins))) +
        geom_histogram(position = "identity", alpha = 0.5, fill = "white") +
        theme_bw()
    }
  )

}

plots <- lapply(top_5_most_correlated, compare_histogram, df = long_df) 
wrap_plots(plots)
```

### VIF: Another Look at Correlation

```{r}
long_df <- select(long_df, -c(TotalGold, TotalMinionsKilled))

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

```{r}
alias(vif_model)
```

```{r}
long_df <- select(long_df, -EliteMonsters)

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

### PCA

```{r}
pca_results <- prcomp(~ . - gameId - Wins, data = long_df, center = TRUE, scale = TRUE)

# scree plot
var_proportion <- pca_results %>%
  summary() %>%
  `[[`("importance") %>%
  t() %>%
  as.data.frame() %>%
  mutate(component_number = str_replace(rownames(.), "PC", "")) %>%
  mutate(component_number = as.numeric(component_number)) %>%
  rename_all(~tolower(str_replace_all(., "\\s+", "_"))) 

scree_plot <- ggplot(var_proportion, aes(x = component_number, y = proportion_of_variance)) +
  geom_col() 

cum_prop_plot <- ggplot(var_proportion, aes(x = component_number, y = cumulative_proportion)) +
  geom_line()

scree_plot + cum_prop_plot
```


```{r}
pca_loadings <- data.frame(variable = rownames(pca_results$rotation), pca_results$rotation)

get_loadings_plot <- function(loadings_df, pc_a, pc_b) {
  wrapr::let(
    alias = list(PCOMP_A = pc_a, PCOMP_B = pc_b),
    expr = {
      ggplot(loadings_df) +
        geom_segment(
          aes(x = 0, y = 0, xend = PCOMP_A, yend = PCOMP_B), 
          arrow = arrow(length = unit(1/2, "picas")),
          color = "black"
        ) +
        annotate(
          "text", 
          x = loadings_df$PCOMP_A*1.2, y = loadings_df$PCOMP_B*1.2,
          label = loadings_df$variable
        ) +
        labs(x = pc_a, y = pc_b)
    }
  )
}

get_loadings_plot(pca_loadings, "PC1", "PC2")
```

```{r}
pca_loadings %>%
  melt(variable.name = "component") %>% 
  ggplot(aes(x = component, y = variable, fill = abs(value))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "white", high = "green", mid = "white",
        midpoint = 0.2, limit = c(0, 1), space = "Lab",
        name = "Loading Value"
    ) +
    theme_minimal() +
    coord_fixed()
```

## Data Wrangling: 1 row per game

```{r}
comparable_cols <- colnames(dataset)[str_detect(colnames(dataset), "red")]
comparable_cols <- str_replace(comparable_cols, "red", "")
comparable_cols <- setdiff(comparable_cols, c("ExperienceDiff", "GoldDiff"))

narrow_df <- dataset

for (col in comparable_cols) {
  
  wrapr::let(
    alias = list(B_VAR = paste0("blue", col), R_VAR = paste0("red", col), D_VAR = paste0("percdiff", col)),
    expr = {
      narrow_df <- narrow_df %>%
        mutate(D_VAR = ifelse(B_VAR == 0 & R_VAR == 0, 0,
                              100 * 2 * (B_VAR - R_VAR) / (B_VAR + R_VAR))
        )
    }
  )
  
}

narrow_df <- cbind(
    select(narrow_df, "gameId", "blueWins"),
    select_at(narrow_df, vars(contains("percdiff")))
  )
```

## Visualization

### Variable Correlation

Next, we look at a correlation heat map with all variables.  There are too many variables to get a clear sense of what's going on, so we'll break it down in the next couple visualizations.
```{r}
cor_mat <- cor(narrow_df)
melted_cormat <- melt(cor_mat, na.rm = TRUE) 
upper_tri <- melted_cormat %>%
  mutate(Var1 = as.character(Var1), Var2 = as.character(Var2)) %>%
  filter(Var1 < Var2) %>%
  mutate(Var1 = as.factor(Var1), Var2 = as.factor(Var2))
# Heatmap
ggplot(data = upper_tri, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

We consider which variables are highly correlated (cor > 0.5).  We see that `TotalGold` and `TotalExperience` are the variables with the most highly correlated pairs.  This makes sense because they are high-level metrics that are likely influenced by lower level metrics such as `AvgLevel` and `GoldDiff`.
```{r}
upper_tri %>%
  filter(abs(value) > 0.5) %>%
  ggplot(aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white")+
  scale_fill_gradient2(
    low = "blue", high = "red", mid = "white", 
    midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation"
  ) +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 9, hjust = 1)) +
  coord_fixed() 
```

Next, we look to see if any variables are 100\% correlated.  We find that `GoldPerMin` is 100\% correlated with `TotalGold` and `CSPerMin` with `TotalMinionsKilled`.  As such we will drop `TotalGold` and `TotalMinionsKilled` when we prepare for modeling.
```{r}
filter(upper_tri, value == 1)
```

### KPI Performance of Winning vs. Losing Team

Our first in-depth look will be the correlation of each variable with Wins.  We find that gold-related KPIs `GoldDiff`, `ExperienceDiff`, `TotalGold`, and `GoldPerMin` top the list.  Experience KPIs `TotalExperience` and `AvgLevel` are slightly less correlated, as are player vs. player aggression KPIs `Kills`, `Deaths`, and `Assists`. Monster, Minion, and Ward KPIs are the least correlated.  It will be interesting to see if our Primary Component Analysis pulls these groups out.
```{r}
as_tibble(cor_mat) %>%
  mutate(comparedVar = rownames(cor_mat)) %>%
  select(comparedVar, blueWins) %>%
  arrange(desc(abs(blueWins)))
``` 

Taking a look at the distributions of the top 5 variables that are most correlated with `Wins`, we see that each distribution is approximately normal.  Additionally, the distribution for the variable when `Wins` is 0 is generally shifted to the left from when `Wins` is 1.  This is in line with out expectations - it makes sense that the losing team should have less Gold and Experience than the winning team.
```{r}
top_5_most_correlated <- c("percdiffGoldPerMin", "percdiffTotalGold", "percdiffTotalExperience", "percdiffKills", "percdiffDeaths")

compare_histogram <- function(df, kpi_name) {
  
  wrapr::let(
    alias = list(KPI_VAR = kpi_name),
    expr = {
      ggplot(df, aes(x = KPI_VAR, color = factor(blueWins))) +
        geom_histogram(position = "identity", alpha = 0.5, fill = "white") +
        theme_bw()
    }
  )

}

plots <- lapply(top_5_most_correlated, compare_histogram, df = narrow_df) 
wrap_plots(plots)
```

### VIF: Another Look at Correlation

```{r}
long_df <- select(long_df, -c(TotalGold, TotalMinionsKilled))

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

```{r}
alias(vif_model)
```

```{r}
long_df <- select(long_df, -EliteMonsters)

vif_model <- lm(Wins ~ . - gameId, data = long_df)

vif(vif_model)
```

### PCA

```{r}
pca_results <- prcomp(~ . - gameId - blueWins, data = narrow_df, center = TRUE, scale = TRUE)

# scree plot
var_proportion <- pca_results %>%
  summary() %>%
  `[[`("importance") %>%
  t() %>%
  as.data.frame() %>%
  mutate(component_number = str_replace(rownames(.), "PC", "")) %>%
  mutate(component_number = as.numeric(component_number)) %>%
  rename_all(~tolower(str_replace_all(., "\\s+", "_"))) 

scree_plot <- ggplot(var_proportion, aes(x = component_number, y = proportion_of_variance)) +
  geom_col() 

cum_prop_plot <- ggplot(var_proportion, aes(x = component_number, y = cumulative_proportion)) +
  geom_line() +
  geom_point() +
  geom_hline(yintercept = 0.9)

scree_plot + cum_prop_plot
```


```{r}
pca_loadings <- data.frame(variable = rownames(pca_results$rotation), pca_results$rotation)

get_loadings_plot <- function(loadings_df, pc_a, pc_b) {
  wrapr::let(
    alias = list(PCOMP_A = pc_a, PCOMP_B = pc_b),
    expr = {
      ggplot(loadings_df) +
        geom_segment(
          aes(x = 0, y = 0, xend = PCOMP_A, yend = PCOMP_B), 
          arrow = arrow(length = unit(1/2, "picas")),
          color = "black"
        ) +
        annotate(
          "text", 
          x = loadings_df$PCOMP_A*1.2, y = loadings_df$PCOMP_B*1.2,
          label = loadings_df$variable
        ) +
        labs(x = pc_a, y = pc_b)
    }
  )
}

get_loadings_plot(pca_loadings, "PC1", "PC2")
```

```{r}
pca_loadings %>%
  melt(variable.name = "component") %>% 
  ggplot(aes(x = component, y = variable, fill = abs(value))) +
    geom_tile(color = "white") +
    scale_fill_gradient2(
        low = "white", high = "green", mid = "white",
        midpoint = 0.2, limit = c(0, 1), space = "Lab",
        name = "Loading Value"
    ) +
    theme_minimal() +
    coord_fixed()
```


{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f16930bc37148e0ae6f29649db789d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1586136531826_0001</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-24-235.us-west-2.compute.internal:20888/proxy/application_1586136531826_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-21-172.us-west-2.compute.internal:8042/node/containerlogs/container_1586136531826_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.ml.feature._\n",
      "import org.apache.spark.ml.classification.{NaiveBayes, NaiveBayesModel, RandomForestClassifier}\n",
      "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
      "import org.apache.spark.sql._\n",
      "import org.apache.spark.sql.expressions.Window\n",
      "import org.apache.spark.ml._\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.{NaiveBayes,NaiveBayesModel,RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20331d523ac547bb8a4a85e0dabe187a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.pyspark.python,python3)\n",
      "(spark.yarn.dist.archives,file:/usr/lib/spark/R/lib/sparkr.zip#sparkr)\n",
      "(spark.sql.warehouse.dir,hdfs:///user/spark/warehouse)\n",
      "(spark.sql.parquet.fs.optimized.committer.optimization-enabled,true)\n",
      "(spark.executor.extraJavaOptions,-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p')\n",
      "(spark.driver.host,ip-172-31-24-235.us-west-2.compute.internal)\n",
      "(spark.history.fs.logDirectory,hdfs:///var/log/spark/apps)\n",
      "(spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem,2)\n",
      "(spark.eventLog.enabled,true)\n",
      "(spark.livy.spark_major_version,2)\n",
      "(spark.driver.port,38183)\n",
      "(spark.shuffle.service.enabled,true)\n",
      "(spark.driver.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)\n",
      "(spark.executorEnv.PYTHONPATH,{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip)\n",
      "(spark.repl.class.uri,spark://ip-172-31-24-235.us-west-2.compute.internal:38183/classes)\n",
      "(spark.jars,file:/usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar,file:/usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:/usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar,file:/usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar,file:/usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar,file:/usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar)\n",
      "(spark.yarn.historyServer.address,ip-172-31-24-235.us-west-2.compute.internal:18080)\n",
      "(spark.stage.attempt.ignoreOnDecommissionFetchFailure,true)\n",
      "(spark.repl.class.outputDir,/tmp/spark536104944988015685)\n",
      "(spark.app.name,livy-session-0)\n",
      "(spark.pyspark.virtualenv.bin.path,/usr/bin/virtualenv)\n",
      "(spark.pyspark.virtualenv.enabled,true)\n",
      "(spark.driver.memory,2048M)\n",
      "(spark.files.fetchFailure.unRegisterOutputOnHost,true)\n",
      "(spark.submit.pyFiles,/usr/lib/spark/python/lib/pyspark.zip,/usr/lib/spark/python/lib/py4j-0.10.7-src.zip)\n",
      "(spark.yarn.secondary.jars,livy-api-0.6.0-incubating.jar,netty-all-4.1.17.Final.jar,livy-rsc-0.6.0-incubating.jar,commons-codec-1.9.jar,livy-core_2.11-0.6.0-incubating.jar,livy-repl_2.11-0.6.0-incubating.jar)\n",
      "(spark.resourceManager.cleanupExpiredHost,true)\n",
      "(spark.executor.id,driver)\n",
      "(spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS,$(hostname -f))\n",
      "(spark.sql.emr.internal.extensions,com.amazonaws.emr.spark.EmrSparkSessionExtensions)\n",
      "(spark.driver.extraJavaOptions,-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p')\n",
      "(spark.yarn.dist.pyFiles,file:///usr/lib/spark/python/lib/pyspark.zip,file:///usr/lib/spark/python/lib/py4j-0.10.7-src.zip)\n",
      "(spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds,2000)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.yarn.maxAppAttempts,1)\n",
      "(spark.master,yarn)\n",
      "(spark.sql.parquet.output.committer.class,com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter)\n",
      "(spark.yarn.tags,livy-session-0-W0hDJzpL)\n",
      "(spark.ui.filters,org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter)\n",
      "(spark.blacklist.decommissioning.timeout,1h)\n",
      "(spark.executor.extraLibraryPath,/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native)\n",
      "(spark.sql.hive.metastore.sharedPrefixes,com.amazonaws.services.dynamodbv2)\n",
      "(spark.executor.memory,9486M)\n",
      "(spark.yarn.submit.waitAppCompletion,false)\n",
      "(spark.driver.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)\n",
      "(spark.eventLog.dir,hdfs:///var/log/spark/apps)\n",
      "(spark.dynamicAllocation.enabled,true)\n",
      "(spark.executor.extraClassPath,/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar)\n",
      "(spark.sql.catalogImplementation,hive)\n",
      "(spark.executor.cores,4)\n",
      "(spark.history.ui.port,18080)\n",
      "(spark.driver.appUIAddress,http://ip-172-31-24-235.us-west-2.compute.internal:4040)\n",
      "(spark.repl.local.jars,file:///usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar,file:///usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar,file:///usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar)\n",
      "(spark.yarn.isPython,true)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS,ip-172-31-24-235.us-west-2.compute.internal)\n",
      "(spark.blacklist.decommissioning.enabled,true)\n",
      "(spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES,http://ip-172-31-24-235.us-west-2.compute.internal:20888/proxy/application_1586136531826_0001)\n",
      "(spark.decommissioning.timeout.threshold,20)\n",
      "(spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem,true)\n",
      "(spark.yarn.dist.jars,file:///usr/lib/livy/rsc-jars/livy-api-0.6.0-incubating.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.6.0-incubating.jar,file:///usr/lib/livy/repl_2.11-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.11-jars/livy-core_2.11-0.6.0-incubating.jar,file:///usr/lib/livy/repl_2.11-jars/livy-repl_2.11-0.6.0-incubating.jar)\n",
      "(spark.app.id,application_1586136531826_0001)\n",
      "(spark.hadoop.yarn.timeline-service.enabled,false)\n",
      "(spark.pyspark.virtualenv.type,native)\n",
      "(spark.yarn.executor.memoryOverheadFactor,0.1875)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kokes.github.io/blog/2018/05/19/spark-sane-csv-processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customSchema = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
       "df = [PostId: double, PostCre...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCre..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"train-sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of training data set in from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e23c59dcf374e0abc93a8c1d589a01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types._\n",
      "customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
      "df: org.apache.spark.sql.DataFrame = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"s3://stackoverflow-kaggle/data/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3.3 million observations across 15 different columns.  However, identifiers such as `PostId` and `OwnerUserId` will be discarded.  `OpenStatus` and `PostClosedDate` will also be discarded as features since the former is our response variable and the latter implies the response value.  That leaves 11 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe402a565a914eb19bbfdb27bcdf5ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res8: Long = 3370528\n"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8825ab5a97f43c8b8422cb8c96ec9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+--------+------------+-----------------+----+----+-------------------+-----------------+\n",
      "|   PostId|   PostCreationDate|OwnerUserId|  OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|               Title|        BodyMarkdown|    Tag1|        Tag2|             Tag3|Tag4|Tag5|     PostClosedDate|       OpenStatus|\n",
      "+---------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+--------+------------+-----------------+----+----+-------------------+-----------------+\n",
      "|      4.0|07/31/2008 21:42:52|        8.0|07/31/2008 21:33:24|                     1.0|                                0.0|  Decimal vs Double?|I'm new to C#, an...|      c#|        null|             null|null|null|               null|            open\n",
      "|      6.0|07/31/2008 22:08:08|        9.0|07/31/2008 21:35:26|                     1.0|                                0.0|Percentage width ...|I've got an absol...|    html|         css|             null|null|null|               null|            open\n",
      "|      8.0|07/31/2008 23:33:19|        9.0|07/31/2008 21:35:26|                    16.0|                                1.0|Tools for porting...|Are there any con...|      j#|        null|             null|null|null|               null|            open\n",
      "|      9.0|07/31/2008 23:40:59|        1.0|07/31/2008 14:22:31|                     1.0|                                1.0|How do I calculat...|Given a DateTime ...|      c#|        null|             null|null|null|               null|            open\n",
      "|9610539.0|03/07/2012 23:07:09|  1021610.0|10/31/2011 08:26:49|                    29.0|                                0.0|retrieve data fro...|I save values of ...|  iphone| objective-c|             ios5|null|null|               null|            open\n",
      "|     39.0|08/01/2008 12:43:11|       33.0|08/01/2008 12:32:10|                     1.0|                                0.0|Reliable Timer in...|I am aware in dot...|      c#|      vb.net|            timer|null|null|               null|            open\n",
      "|     19.0|08/01/2008 05:21:22|       13.0|08/01/2008 04:18:04|                     1.0|                                0.0|Fastest way to ge...|Solutions welcome...|      pi|       speed|language-agnostic|unix|null|               null|            open\n",
      "|     23.0|08/01/2008 12:09:41|       48.0|08/01/2008 13:25:15|                     1.0|                                0.0|Latest informatio...|I'm trying to tra...|     php|        null|             null|null|null|05/18/2012 11:12:42|not constructive\n",
      "|     24.0|08/01/2008 12:12:19|       22.0|08/01/2008 12:11:11|                     1.0|                                0.0|Throw Error In My...|If I have a trigg...|   mysql|        null|             null|null|null|               null|            open\n",
      "|     25.0|08/01/2008 12:13:50|       23.0|08/01/2008 12:11:43|                     1.0|                                0.0|How to use the C ...|I've been having ...|     c++|           c|          sockets| zos|null|               null|            open\n",
      "|     11.0|07/31/2008 23:55:37|        1.0|07/31/2008 14:22:31|                     1.0|                                2.0|How do I calculat...|Given a specific ...|      c#|        null|             null|null|null|               null|            open\n",
      "|     13.0|08/01/2008 00:42:38|        9.0|07/31/2008 21:35:26|                    16.0|                                1.0|Determining web u...|Is there a standa...|    html|     browser|             null|null|null|               null|            open\n",
      "|     14.0|08/01/2008 00:59:11|       11.0|08/01/2008 00:59:11|                     1.0|                                0.0|What's the differ...|What is the diffe...|      c#|        null|             null|null|null|               null|            open\n",
      "|     16.0|08/01/2008 04:59:33|        2.0|07/31/2008 14:22:31|                     1.0|                                0.0|How do I fill a D...|How do you expose...|    linq| web-service|               c#|null|null|               null|            open\n",
      "|     17.0|08/01/2008 05:09:55|        2.0|07/31/2008 14:22:31|                     1.0|                                0.0|Binary Data in MYSQL|How do I store bi...|database|       mysql|             null|null|null|               null|            open\n",
      "|     42.0|08/01/2008 12:50:18|       37.0|08/01/2008 12:44:00|                     1.0|                                0.0|Best way to allow...|I am starting a n...|     php|architecture|         plug-ins|null|null|               null|            open\n",
      "|     48.0|08/01/2008 13:01:17|       40.0|08/01/2008 12:48:12|                     1.0|                                0.0|Multiple submit b...|Let's say you cre...|    html|       forms|             null|null|null|               null|            open\n",
      "|     59.0|08/01/2008 13:14:33|       45.0|08/01/2008 13:04:45|                     1.0|                                0.0|How do I get a di...|Lets say I have a...|      c#|        linq|             null|null|null|               null|            open\n",
      "|     66.0|08/01/2008 13:20:46|       17.0|08/01/2008 12:02:21|                     1.0|                                5.0|How do you page a...|How do you page t...|    linq|        null|             null|null|null|               null|            open\n",
      "|     72.0|08/01/2008 13:38:27|       25.0|08/01/2008 12:15:23|                     1.0|                                1.0|How do I add exis...|I've got all thes...|    ruby|        null|             null|null|null|               null|            open\n",
      "+---------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+--------+------------+-----------------+----+----+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OpenStatus` column seems to have an issue with trailing whitespace -- there should be a column ending pipe operator `|` in the printed table above.  Using `trim` didn't work, so we're taking a regex approach instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36d6a1206204aa8b232b7cfed2ca6ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"OpenStatus\", regexp_extract(col(\"OpenStatus\"), \"([\\\\w\\\\s]+\\\\w)\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the percentage of null values for each column, we see that `Tag1` is rarely missing but that the other `TagX` fields increase in sparsity as `X` increases.  `Tag4` and `Tag5` will likely need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc0b6f2e391a46e9af9c0067a1f513a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+--------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+----------+\n",
      "|PostId|PostCreationDate|OwnerUserId|OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|Title|        BodyMarkdown|                Tag1|               Tag2|              Tag3|              Tag4|              Tag5|    PostClosedDate|OpenStatus|\n",
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+--------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+----------+\n",
      "|   0.0|             0.0|        0.0|              0.0|                     0.0|                                0.0|  0.0|2.966894207673100...|5.933788415346201E-7|0.15554002221610383|0.4118055094038679|0.6906570721263849|0.8798034017222227|0.9791913907850639|       0.0|\n",
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+--------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.\n",
    "    select(df.columns.map(c => (sum(when(col(c).isNull || col(c) === \"\" || col(c).isNaN, 1).otherwise(0)) / df.count()).alias(c)): _*).\n",
    "    show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the `Tag` fields with \"unknown\" so they can be processed as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514d1a0097494a8a959a54c66d2c6fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: org.apache.spark.sql.DataFrame = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "df = df.na.fill(\"unknown\", Seq(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Naive Bayes with Tags as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll take a simple approach - encode the Tag columns as categorical variables and predict `OpenStatus` using Random Forest. Spark's RF Classifier example will serve as a reference ([link](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b1f1b808ac8472f9deec0fc4f63da0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_simple: org.apache.spark.sql.DataFrame = [Tag1: string, Tag2: string ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - frequency of `OpenStatus` and `TagX`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the label `OpenStatus` has only 5 distinct values while each of the tags has over 20,000 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34246e5d723a4a58aa06e3cb8290754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Tag1 values: 20061\n",
      "Distinct Tag2 values: 30029\n",
      "Distinct Tag3 values: 32956\n",
      "Distinct Tag4 values: 31245\n",
      "Distinct Tag5 values: 27852\n",
      "Distinct OpenStatus values: 5\n"
     ]
    }
   ],
   "source": [
    "df_simple.\n",
    "    columns.\n",
    "    map(c => {\n",
    "            val count = df_simple.select(c).distinct.count\n",
    "            f\"Distinct $c values: $count\"\n",
    "    }).\n",
    "    foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OpenStatus` takes on the value `open` in nearly all of the cases.  Each of the remaining categories appears in less than 1% of the observations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a90c8b9e94408d8014c932ee71f68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status_counts: org.apache.spark.sql.DataFrame = [OpenStatus: string, count: bigint ... 1 more field]\n",
      "+-------------------+-------+-------------------+\n",
      "|         OpenStatus|  count|      perc_of_total|\n",
      "+-------------------+-------+-------------------+\n",
      "|               open|3300392|  97.91913907850639|\n",
      "|not a real question|  30789| 0.9134770576004709|\n",
      "|          off topic|  17530| 0.5200965546050945|\n",
      "|   not constructive|  15659| 0.4645859639795308|\n",
      "|      too localized|   6158|0.18270134530850954|\n",
      "+-------------------+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val status_counts = df_simple.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    sort(col(\"count\").desc).\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / df_simple.count())\n",
    "\n",
    "status_counts.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cut down on the number of distinct `TagX` values, we'll try keeping the top 90% of observations.  The bottom 10% will be replaced with \"other\".  Below we count the number of distinct tags in the top 90% of observations for each `TagX` column.  `Tag2` and `Tag3` still have over 500 distinct values, but this is far less than the 10,000+ values they originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9631d203b004682b5d33b317ed41bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_percentile_count: (df: org.apache.spark.sql.DataFrame, percentile: Double)(col_name: String)Double\n",
      "res15: Array[Double] = Array(340.0, 2026.0, 3192.0, 2400.0, 305.0)\n"
     ]
    }
   ],
   "source": [
    "def get_percentile_count(df: DataFrame, percentile: Double)(col_name: String): Double = {\n",
    "    \n",
    "    val cumsum_window = Window.\n",
    "      orderBy(col(\"count\").desc).\n",
    "      rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    val total_window = Window.\n",
    "        rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df.\n",
    "        groupBy(col_name).\n",
    "        count().\n",
    "        orderBy(col(\"count\").desc).\n",
    "        withColumn(\"fracObs\", sum(col(\"count\")).over(cumsum_window) / sum(col(\"count\")).over(total_window)).\n",
    "        filter(col(\"fracObs\") <= percentile).\n",
    "        count()\n",
    "}\n",
    "\n",
    "Array(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\").\n",
    "    map(get_percentile_count(df_simple, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are null `Tag1` values, we use `setHandleInvalid(\"keep\")` so that they are indexed rather than dropped ([so_link](https://stackoverflow.com/a/36113473))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a StringIndexer for each Tag column; rather than create 5 variables we'll take a functional approach.  Note that `setHandleInvalid` is set to \"keep\" so that the indexer adds new indexes when it sees new labels in data sets other than our current data set ([StackOverflow link](https://stackoverflow.com/a/43917703/11407644))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(Tag1, Tag2, Tag3, Tag4, Tag5)\n",
       "featureIndexers = Array(strIdx_611619faad8a, strIdx_cb5e9c7716c2, strIdx_cfcab24b941c, strIdx_410970f4bcd3, strIdx_77761dfab371)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(strIdx_611619faad8a, strIdx_cb5e9c7716c2, strIdx_cfcab24b941c, strIdx_410970f4bcd3, strIdx_77761dfab371)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = Array[String](\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\")\n",
    "\n",
    "val featureIndexers = featureCols.map { colName =>\n",
    "    new StringIndexer().\n",
    "        setInputCol(colName).\n",
    "        setOutputCol(\"indexed\" + colName).\n",
    "        setHandleInvalid(\"keep\").\n",
    "        fit(df_simple)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML models expect a feature vector to be the only predictor.  [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is a transformer that combines a list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_1d91282ebf85\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_1d91282ebf85"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureCols.map{x => \"indexed\" + x}).\n",
    "    setOutputCol(\"features\").\n",
    "    setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the categorical features, we index the response.  Keeping with convention, the indexed response is called `indexedLabel` rather than `indexedOpenStatus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_b539aaa6ac67\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "strIdx_b539aaa6ac67"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().\n",
    "    setInputCol(\"OpenStatus\").\n",
    "    setOutputCol(\"indexedLabel\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(df_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the response is indexed, we need a way to transform the predicted response back to its original string value.  This inverse transformer is called [`IndexToString`](https://spark.apache.org/docs/latest/ml-features.html#indextostring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelConverter = idxToStr_aaba441fd44a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idxToStr_aaba441fd44a"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelConverter = new IndexToString().\n",
    "    setInputCol(\"prediction\").\n",
    "    setOutputCol(\"predictionLabel\").\n",
    "    setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can specify our model, multinomial [NaiveBayes](https://spark.apache.org/docs/latest/mllib-naive-bayes.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb = nb_b39000a2c562\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nb_b39000a2c562"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb = new NaiveBayes().\n",
    "    setLabelCol(\"label\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setSmoothing(1.0).\n",
    "    setModelType(\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is constructed by combining the feature indexers built for each of the `TagX` variables with the vector assember, label index, and model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_pipeline = pipeline_300358b41c3d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_300358b41c3d"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, nb, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can specify our model, a [`RandomForestClassifer`](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe20f2a3e35441efa484a05034bce0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_9df6a58d40ea\n"
     ]
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier().\n",
    "    setLabelCol(\"label\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setNumTrees(10).\n",
    "    setMaxBins(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b925ac37d2a4dd28c5874f873c5d16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_b75699e47b54\n"
     ]
    }
   ],
   "source": [
    "val rf_pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [Tag1: string, Tag2: string ... 4 more fields]\n",
       "test = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = df_simple.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to the training set to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+--------------------+\n",
      "|predictionLabel|label|            features|\n",
      "+---------------+-----+--------------------+\n",
      "|      off topic|  0.0|    (5,[0],[3312.0])|\n",
      "|      off topic|  0.0|[75.0,552.0,619.0...|\n",
      "|  too localized|  0.0|[75.0,285.0,504.0...|\n",
      "|           open|  2.0|[75.0,342.0,10612...|\n",
      "|      off topic|  0.0|(5,[0,1],[75.0,23...|\n",
      "+---------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model = pipeline_300358b41c3d\n",
       "predictions = [Tag1: string, Tag2: string ... 15 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 15 more fields]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = nb_pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(test)\n",
    "predictions.select(\"predictionLabel\", \"OpenStatus\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up [MulticlassClassificationEvaluator](https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_28b51846e9f0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_28b51846e9f0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy was ~27%.  To get a sense for how good or bad this is, let's look at how well the model would have performed if it simply guessed the most common response (`Open`) each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy = 0.27433144919963903\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.27433144919963903"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|         OpenStatus|count|     perc_of_total|\n",
      "+-------------------+-----+------------------+\n",
      "|not a real question| 9334|22.167862062413906|\n",
      "|      too localized| 1842| 4.374673443214744|\n",
      "|   not constructive| 4692|11.143304992162637|\n",
      "|          off topic| 5274|12.525530803210943|\n",
      "|               open|20964| 49.78862869899777|\n",
      "+-------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / lit(predictions.count())).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model had guessed `open` for each test observation, the accuracy would have been 49.8% -- much higher than the 27% that NaiveBayes achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|         OpenStatus|           accurracy|\n",
      "+-------------------+--------------------+\n",
      "|not a real question|0.030319262909792158|\n",
      "|      too localized| 0.05863192182410423|\n",
      "|   not constructive|0.017263427109974423|\n",
      "|          off topic|   0.586841107318923|\n",
      "|               open|  0.3808433505056287|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    select(\"predictionLabel\", \"OpenStatus\").\n",
    "    withColumn(\"success\", when(col(\"predictionLabel\") === col(\"OpenStatus\"), 1).otherwise(0)).\n",
    "    groupBy(\"OpenStatus\").\n",
    "    agg((sum(col(\"success\")) / count(\"*\")).alias(\"accurracy\")).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at class-specific performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

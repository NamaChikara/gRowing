{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ea5d59fcef42e19d3a161bdcccd8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<console>:41: error: not found: value kernel\n",
      "        kernel.publish.html(s\"\"\" <table>\n",
      "        ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "implicit class RichDF(val ds:DataFrame) {\n",
    "    def showHTML(limit:Int = 20, truncate: Int = 20) = {\n",
    "        import xml.Utility.escape\n",
    "        val data = ds.take(limit)\n",
    "        val header = ds.schema.fieldNames.toSeq        \n",
    "        val rows: Seq[Seq[String]] = data.map { row =>\n",
    "          row.toSeq.map { cell =>\n",
    "            val str = cell match {\n",
    "              case null => \"null\"\n",
    "              case binary: Array[Byte] => binary.map(\"%02X\".format(_)).mkString(\"[\", \" \", \"]\")\n",
    "              case array: Array[_] => array.mkString(\"[\", \", \", \"]\")\n",
    "              case seq: Seq[_] => seq.mkString(\"[\", \", \", \"]\")\n",
    "              case _ => cell.toString\n",
    "            }\n",
    "            if (truncate > 0 && str.length > truncate) {\n",
    "              // do not show ellipses for strings shorter than 4 characters.\n",
    "              if (truncate < 4) str.substring(0, truncate)\n",
    "              else str.substring(0, truncate - 3) + \"...\"\n",
    "            } else {\n",
    "              str\n",
    "            }\n",
    "          }: Seq[String]\n",
    "        }\n",
    "\n",
    "        kernel.publish.html(s\"\"\" <table>\n",
    "                <tr>\n",
    "                 ${header.map(h => s\"<th>${escape(h)}</th>\").mkString}\n",
    "                </tr>\n",
    "                ${rows.map { row =>\n",
    "                  s\"<tr>${row.map{c => s\"<td>${escape(c)}</td>\" }.mkString}</tr>\"\n",
    "                }.mkString}\n",
    "            </table>\n",
    "        \"\"\")        \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.expressions.Window$@5de0f7ad"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions.Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kokes.github.io/blog/2018/05/19/spark-sane-csv-processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customSchema = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
       "df = [PostId: double, PostCre...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCre..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"train-sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of training data set in from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6206ddd6e3ae4810b4ced27a856ccc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.types._\n",
      "customSchema: org.apache.spark.sql.types.StructType = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
      "df: org.apache.spark.sql.DataFrame = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"s3://stackoverflow-kaggle/data/train-sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+---------------+-----------------+-------------+------------------+----------+-------------------+--------------------+\n",
      "|     PostId|   PostCreationDate|OwnerUserId|  OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|               Title|        BodyMarkdown|           Tag1|             Tag2|         Tag3|              Tag4|      Tag5|     PostClosedDate|          OpenStatus|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+---------------+-----------------+-------------+------------------+----------+-------------------+--------------------+\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+---------------+-----------------+-------------+------------------+----------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the percentage of null values for each column -- Tag2, Tag3, and Tag4 are the only columns with a significant amount of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"OpenStatus\", regexp_extract(col(\"OpenStatus\"), \"([\\\\w\\\\s]+\\\\w)\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+------------+--------------------+-------------------+-------------------+------------------+------------------+--------------+----------+\n",
      "|PostId|PostCreationDate|OwnerUserId|OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|Title|BodyMarkdown|                Tag1|               Tag2|               Tag3|              Tag4|              Tag5|PostClosedDate|OpenStatus|\n",
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+------------+--------------------+-------------------+-------------------+------------------+------------------+--------------+----------+\n",
      "|   0.0|             0.0|        0.0|              0.0|                     0.0|                                0.0|  0.0|         0.0|7.129006501653929E-6|0.19408720200752824|0.45855195619938405|0.7172208281053952|0.8879534048135052|           0.5|       0.0|\n",
      "+------+----------------+-----------+-----------------+------------------------+-----------------------------------+-----+------------+--------------------+-------------------+-------------------+------------------+------------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.\n",
    "    select(df.columns.map(c => (sum(when(col(c).isNull || col(c) === \"\" || col(c).isNaN, 1).otherwise(0)) / df.count()).alias(c)): _*).\n",
    "    show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.fill(\"unknown\", Seq(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Random Forest with Tags as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll take a simple approach - encode the Tag columns as categorical variables and predict `OpenStatus` using Random Forest. Spark's RF Classifier example will serve as a reference ([link](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_simple = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - frequency of `OpenStatus` and `Tag1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `null` is the most often observed response by a large margin --- our data set is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|         OpenStatus|count|\n",
      "+-------------------+-----+\n",
      "|               open|70136|\n",
      "|not a real question|30789|\n",
      "|          off topic|17530|\n",
      "|   not constructive|15659|\n",
      "|      too localized| 6158|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status_counts = [OpenStatus: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OpenStatus: string, count: bigint]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val status_counts = df_simple.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    sort(col(\"count\").desc)\n",
    "\n",
    "status_counts.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out pretty much every null `OpenStatus` is paired with a null `Tag1` (at leas in our sample training set).\n",
    "\n",
    "Note: in the join the use of `<=>` allows for joining on null values of `tag1` ([so_link](https://stackoverflow.com/a/41729359))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----+----------+\n",
      "|   Tag1|         OpenStatus|count|tag1_count|\n",
      "+-------+-------------------+-----+----------+\n",
      "|    php|               open| 4781|     12499|\n",
      "|    php|not a real question| 4185|     12499|\n",
      "|    php|   not constructive| 1485|     12499|\n",
      "|    php|          off topic| 1032|     12499|\n",
      "|    php|      too localized| 1016|     12499|\n",
      "|     c#|               open| 5831|     11322|\n",
      "|     c#|not a real question| 3372|     11322|\n",
      "|     c#|   not constructive| 1194|     11322|\n",
      "|     c#|      too localized|  502|     11322|\n",
      "|     c#|          off topic|  423|     11322|\n",
      "|   java|               open| 4923|     11146|\n",
      "|   java|not a real question| 3499|     11146|\n",
      "|   java|   not constructive| 1562|     11146|\n",
      "|   java|          off topic|  588|     11146|\n",
      "|   java|      too localized|  574|     11146|\n",
      "|android|               open| 3735|      7175|\n",
      "|android|not a real question| 1974|      7175|\n",
      "|android|          off topic|  761|      7175|\n",
      "|android|   not constructive|  463|      7175|\n",
      "|android|      too localized|  242|      7175|\n",
      "+-------+-------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tag1_counts = [Tag1: string, tag1_count: bigint]\n",
       "tag1_status = [Tag1: string, OpenStatus: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, OpenStatus: string ... 2 more fields]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tag1_counts = df_simple.\n",
    "    groupBy(\"Tag1\").\n",
    "    count().\n",
    "    withColumnRenamed(\"count\", \"tag1_count\").\n",
    "    orderBy(col(\"count\").desc)\n",
    "\n",
    "val tag1_status = df_simple.\n",
    "    groupBy(\"Tag1\", \"OpenStatus\").\n",
    "    count().\n",
    "    join(tag1_counts, Seq(\"Tag1\"), joinType = \"left\").\n",
    "    where(col(\"count\") > 20).\n",
    "    orderBy(col(\"tag1_count\").desc_nulls_first, col(\"count\").desc_nulls_first)\n",
    "\n",
    "tag1_status.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cumsum_window = org.apache.spark.sql.expressions.WindowSpec@7c6f4283\n",
       "total_window = org.apache.spark.sql.expressions.WindowSpec@4e033066\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cumsum_window = Window.\n",
    "  orderBy(col(\"count\").desc).\n",
    "  rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "val total_window = Window.\n",
    "    rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "df_simple.\n",
    "    groupBy(\"Tag1\").\n",
    "    count().\n",
    "    orderBy(col(\"count\").desc).\n",
    "    withColumn(\"fracObs\", sum(col(\"count\")).over(cumsum_window) / sum(col(\"count\")).over(total_window)).\n",
    "    filter(col(\"fracObs\") <= 0.90).\n",
    "    count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are null `Tag1` values, we use `setHandleInvalid(\"keep\")` so that they are indexed rather than dropped ([so_link](https://stackoverflow.com/a/36113473))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c825c6ca06f84a83be5272f0374b1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag1Indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_ba0487a209f3\n"
     ]
    }
   ],
   "source": [
    "val tag1Indexer = new StringIndexer().\n",
    "    setInputCol(\"Tag1\").\n",
    "    setOutputCol(\"indexedTag1\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67380b24c38c4b1d85b46bef0262fe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Tag1|indexedTag1|\n",
      "+----+-----------+\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tag1Indexer.\n",
    "    transform(simple_train).\n",
    "    select(\"Tag1\", \"indexedTag1\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a StringIndexer for each Tag column; rather than create 5 variables we'll take a functional approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d7a1aaec15467194bfe45c619a3b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureCols: Array[String] = Array(Tag1, Tag2, Tag3, Tag4, Tag5)\n",
      "featureIndexers: Array[org.apache.spark.ml.feature.StringIndexerModel] = Array(strIdx_3f2aedc88dcd, strIdx_b80161de7b98, strIdx_285c550b6cee, strIdx_a152531a9eae, strIdx_db6635162996)\n"
     ]
    }
   ],
   "source": [
    "val featureCols = Array[String](\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\")\n",
    "\n",
    "val featureIndexers = featureCols.map { colName =>\n",
    "    new StringIndexer().\n",
    "        setInputCol(colName).\n",
    "        setOutputCol(\"indexed\" + colName).\n",
    "        setHandleInvalid(\"keep\").\n",
    "        fit(simple_train)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML models expect a feature vector to be the only predictor.  [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is a transformer that combines a list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b035a9fbf574d229ea535af6728451b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_47f54c17f830\n"
     ]
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureCols.map{x => \"indexed\" + x}).\n",
    "    setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the categorical features, we index the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34459010fd4a4147a073a452eaae9717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_763c47d16a69\n"
     ]
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().\n",
    "    setInputCol(\"OpenStatus\").\n",
    "    setOutputCol(\"label\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(simple_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the response is indexed, we need a way to transform the predicted response back to its original string value.  This inverse transformer is called [`IndexToString`](https://spark.apache.org/docs/latest/ml-features.html#indextostring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b55e59c0e7f4337b28c5a35b4611b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_16829d1c7c43\n"
     ]
    }
   ],
   "source": [
    "val labelConverter = new IndexToString().\n",
    "    setInputCol(\"prediction\").\n",
    "    setOutputCol(\"predictionLabel\").\n",
    "    setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can specify our model, a [`RandomForestClassifer`](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe20f2a3e35441efa484a05034bce0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_9df6a58d40ea\n"
     ]
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier().\n",
    "    setLabelCol(\"label\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setNumTrees(10).\n",
    "    setMaxBins(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b925ac37d2a4dd28c5874f873c5d16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_b75699e47b54\n"
     ]
    }
   ],
   "source": [
    "val pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbd0879bc454f0e9a08322275474e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Tag1: string, Tag2: string ... 4 more fields]\n",
      "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Tag1: string, Tag2: string ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "val Array(trainingData, testData) = simple_train.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a88502366bb4990b99f0805604976f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.ml.PipelineModel = pipeline_b75699e47b54\n"
     ]
    }
   ],
   "source": [
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ddadb8812c438bacc4755b189b3371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [Tag1: string, Tag2: string ... 15 more fields]\n"
     ]
    }
   ],
   "source": [
    "val predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e307a6677de47f186a53d949f9e352a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 616.0 failed 4 times, most recent failure: Lost task 0.3 in stage 616.0 (TID 6026, ip-172-31-8-106.us-west-2.compute.internal, executor 26): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$10: (double) => string)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Unseen index: 1168.0 ??\n",
      "\tat org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:404)\n",
      "\tat org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:399)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n",
      "  ... 64 elided\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$10: (double) => string)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "  ... 3 more\n",
      "Caused by: org.apache.spark.SparkException: Unseen index: 1168.0 ??\n",
      "  at org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:404)\n",
      "  at org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:399)\n",
      "  ... 21 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predictionLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f7edc32434dc0bff180ce1d4c6296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a06798eca4b7\n"
     ]
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"indexedLabel\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

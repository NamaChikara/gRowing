{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_small = [_c0: string, _c1: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: string, _c1: string ... 13 more fields]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_small = spark.read.csv(\"train-sample-small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customSchema = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
       "temp = [PostId: double, PostC...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostC..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "val temp = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"header\", \"true\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"train-sample-small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14027"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|         OpenStatus|count|\n",
      "+-------------------+-----+\n",
      "|               open| 7075|\n",
      "|not a real question| 3049|\n",
      "|          off topic| 1750|\n",
      "|   not constructive| 1547|\n",
      "|      too localized|  606|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp.    \n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    orderBy(col(\"count\").desc).\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of training data set in from S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.io.IOException\n",
       "Message: No FileSystem for scheme: s3\n",
       "StackTrace:   at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n",
       "  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n",
       "  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n",
       "  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n",
       "  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:547)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:545)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:392)\n",
       "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.flatMap(List.scala:355)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:545)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:359)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:619)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:468)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_small = spark.\n",
    "    read.\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    csv(\"s3://stackoverflow-kaggle/data/train-sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----+----+----+----+----+--------------+----------+\n",
      "|              PostId|    PostCreationDate|OwnerUserId|  OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|               Title|        BodyMarkdown|Tag1|Tag2|Tag3|Tag4|Tag5|PostClosedDate|OpenStatus|\n",
      "+--------------------+--------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----+----+----+----+----+--------------+----------+\n",
      "|             2221995| 02/08/2010 13:57:35|      60200|01/29/2009 13:43:43|                     260|                                 11|3d object visible...|\"I'm playing arou...|null|null|null|null|null|          null|      null|\n",
      "|I want to just sh...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|           sometimes| I want to just s...|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|`How can I do hid...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|    I'm found answer| but this tell \"\"...|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|`Is it possible t...|                 wpf|         c#|                 NA|                      NA|                                 NA|                  NA|                open|null|null|null|null|null|          null|      null|\n",
      "|              269316| 11/06/2008 16:26:17|      32372|10/29/2008 14:06:48|                      15|                                  0|Auto-Generate Bin...|In Visual Studio ...|null|null|null|null|null|          null|      null|\n",
      "|    <asp:DetailsV...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|  </asp:DetailsView>|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|<asp:ObjectDataSo...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|    <SelectParame...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|        <asp:Quer...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|    </SelectParam...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|</asp:ObjectDataS...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|I am not sure how...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|          Any ideas?|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|,asp.net,visual-s...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|             9207326| 02/09/2012 08:09:01|     244651|01/06/2010 11:24:08|                      49|                                  2|Using a humongous...|I am trying to us...|null|null|null|null|null|          null|      null|\n",
      "|My question is if...|                null|       null|               null|                    null|                               null|                null|                null|null|null|null|null|null|          null|      null|\n",
      "|I've noticed that...| where is it gett...|    android|             sqlite|                      NA|                                 NA|                  NA|                  NA|open|null|null|null|null|          null|      null|\n",
      "+--------------------+--------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----+----+----+----+----+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_small.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Random Forest with Tags as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll take a simple approach - encode the Tag columns as categorical variables and predict `OpenStatus` using Random Forest. Spark's RF Classifier example will serve as a reference ([link](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_train = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val simple_train = train_small.\n",
    "        select(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\", \"OpenStatus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - frequency of `OpenStatus` and `Tag1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `null` is the most often observed response by a large margin --- our data set is very imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|          OpenStatus| count|\n",
      "+--------------------+------+\n",
      "|                null|179428|\n",
      "| not a real question|   736|\n",
      "|                open|   578|\n",
      "|           off topic|   335|\n",
      "|    not constructive|   289|\n",
      "|                  NA|    33|\n",
      "|       too localized|    32|\n",
      "|         item_number|     5|\n",
      "|               4(8)>|     2|\n",
      "|org.springframewo...|     1|\n",
      "|              iphone|     1|\n",
      "|       data['Notes']|     1|\n",
      "| 01/06/2012 02:37:00|     1|\n",
      "| 02/21/2011 17:25:58|     1|\n",
      "|                   l|     1|\n",
      "|               '079'|     1|\n",
      "| this can only wo...|     1|\n",
      "|               '{5}'|     1|\n",
      "| 06/14/2012 01:19:12|     1|\n",
      "| 09/17/2011 08:13:57|     1|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status_counts = [OpenStatus: string, count: bigint]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OpenStatus: string, count: bigint]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val status_counts = simple_train.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    orderBy(col(\"count\").desc)\n",
    "\n",
    "status_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out pretty much every null `OpenStatus` is paired with a null `Tag1` (at leas in our sample training set).\n",
    "\n",
    "Note: in the join the use of `<=>` allows for joining on null values of `tag1` ([so_link](https://stackoverflow.com/a/41729359))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a23a8a5c8a4a6b80348c8ba3d57589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag1_counts: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tag1: string, tag1_count: bigint]\n",
      "tag1_status: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [tag1: string, OpenStatus: string ... 3 more fields]\n",
      "+-------------------+-------------------+-------+-------------------+----------+\n",
      "|               tag1|         OpenStatus|  count|               tag1|tag1_count|\n",
      "+-------------------+-------------------+-------+-------------------+----------+\n",
      "|               null|               null|1761529|               null|   1761536|\n",
      "|               open|               null|   6058|               open|      6060|\n",
      "|               java|not a real question|    840|               java|      1709|\n",
      "|               java|               open|    422|               java|      1709|\n",
      "|               java|   not constructive|    313|               java|      1709|\n",
      "|               java|          off topic|    105|               java|      1709|\n",
      "|               java|      too localized|     27|               java|      1709|\n",
      "|not a real question|               null|   1528|not a real question|      1528|\n",
      "|                 c#|not a real question|    780|                 c#|      1467|\n",
      "|                 c#|               open|    363|                 c#|      1467|\n",
      "|                 c#|   not constructive|    221|                 c#|      1467|\n",
      "|                 c#|          off topic|     67|                 c#|      1467|\n",
      "|                 c#|      too localized|     34|                 c#|      1467|\n",
      "|                php|not a real question|    750|                php|      1418|\n",
      "|                php|               open|    255|                php|      1418|\n",
      "|                php|   not constructive|    232|                php|      1418|\n",
      "|                php|          off topic|    144|                php|      1418|\n",
      "|                php|      too localized|     36|                php|      1418|\n",
      "|   not constructive|               null|   1385|   not constructive|      1385|\n",
      "|          off topic|               null|   1354|          off topic|      1354|\n",
      "+-------------------+-------------------+-------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val tag1_counts = simple_train.\n",
    "    groupBy(\"tag1\").\n",
    "    count().\n",
    "    withColumnRenamed(\"count\", \"tag1_count\").\n",
    "    orderBy(col(\"count\").desc)\n",
    "\n",
    "val tag1_status = simple_train.\n",
    "    groupBy(\"tag1\", \"OpenStatus\").\n",
    "    count().\n",
    "    join(tag1_counts, train_small(\"tag1\") <=> tag1_counts(\"tag1\"), joinType = \"left\").\n",
    "    where(col(\"count\") > 20).\n",
    "    orderBy(col(\"tag1_count\").desc_nulls_first, col(\"count\").desc_nulls_first)\n",
    "\n",
    "tag1_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are null `Tag1` values, we use `setHandleInvalid(\"keep\")` so that they are indexed rather than dropped ([so_link](https://stackoverflow.com/a/36113473))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c825c6ca06f84a83be5272f0374b1421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag1Indexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_ba0487a209f3\n"
     ]
    }
   ],
   "source": [
    "val tag1Indexer = new StringIndexer().\n",
    "    setInputCol(\"Tag1\").\n",
    "    setOutputCol(\"indexedTag1\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(simple_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67380b24c38c4b1d85b46bef0262fe02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|Tag1|indexedTag1|\n",
      "+----+-----------+\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "|null|    10609.0|\n",
      "+----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tag1Indexer.\n",
    "    transform(simple_train).\n",
    "    select(\"Tag1\", \"indexedTag1\").\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a StringIndexer for each Tag column; rather than create 5 variables we'll take a functional approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d7a1aaec15467194bfe45c619a3b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureCols: Array[String] = Array(Tag1, Tag2, Tag3, Tag4, Tag5)\n",
      "featureIndexers: Array[org.apache.spark.ml.feature.StringIndexerModel] = Array(strIdx_3f2aedc88dcd, strIdx_b80161de7b98, strIdx_285c550b6cee, strIdx_a152531a9eae, strIdx_db6635162996)\n"
     ]
    }
   ],
   "source": [
    "val featureCols = Array[String](\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\")\n",
    "\n",
    "val featureIndexers = featureCols.map { colName =>\n",
    "    new StringIndexer().\n",
    "        setInputCol(colName).\n",
    "        setOutputCol(\"indexed\" + colName).\n",
    "        setHandleInvalid(\"keep\").\n",
    "        fit(simple_train)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML models expect a feature vector to be the only predictor.  [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is a transformer that combines a list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b035a9fbf574d229ea535af6728451b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_47f54c17f830\n"
     ]
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureCols.map{x => \"indexed\" + x}).\n",
    "    setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the categorical features, we index the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34459010fd4a4147a073a452eaae9717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelIndexer: org.apache.spark.ml.feature.StringIndexerModel = strIdx_763c47d16a69\n"
     ]
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().\n",
    "    setInputCol(\"OpenStatus\").\n",
    "    setOutputCol(\"label\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(simple_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the response is indexed, we need a way to transform the predicted response back to its original string value.  This inverse transformer is called [`IndexToString`](https://spark.apache.org/docs/latest/ml-features.html#indextostring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b55e59c0e7f4337b28c5a35b4611b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelConverter: org.apache.spark.ml.feature.IndexToString = idxToStr_16829d1c7c43\n"
     ]
    }
   ],
   "source": [
    "val labelConverter = new IndexToString().\n",
    "    setInputCol(\"prediction\").\n",
    "    setOutputCol(\"predictionLabel\").\n",
    "    setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can specify our model, a [`RandomForestClassifer`](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe20f2a3e35441efa484a05034bce0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_9df6a58d40ea\n"
     ]
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier().\n",
    "    setLabelCol(\"label\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setNumTrees(10).\n",
    "    setMaxBins(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b925ac37d2a4dd28c5874f873c5d16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline: org.apache.spark.ml.Pipeline = pipeline_b75699e47b54\n"
     ]
    }
   ],
   "source": [
    "val pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdbd0879bc454f0e9a08322275474e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainingData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Tag1: string, Tag2: string ... 4 more fields]\n",
      "testData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Tag1: string, Tag2: string ... 4 more fields]\n"
     ]
    }
   ],
   "source": [
    "val Array(trainingData, testData) = simple_train.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a88502366bb4990b99f0805604976f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: org.apache.spark.ml.PipelineModel = pipeline_b75699e47b54\n"
     ]
    }
   ],
   "source": [
    "val model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17ddadb8812c438bacc4755b189b3371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: org.apache.spark.sql.DataFrame = [Tag1: string, Tag2: string ... 15 more fields]\n"
     ]
    }
   ],
   "source": [
    "val predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e307a6677de47f186a53d949f9e352a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 616.0 failed 4 times, most recent failure: Lost task 0.3 in stage 616.0 (TID 6026, ip-172-31-8-106.us-west-2.compute.internal, executor 26): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$10: (double) => string)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Unseen index: 1168.0 ??\n",
      "\tat org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:404)\n",
      "\tat org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:399)\n",
      "\t... 21 more\n",
      "\n",
      "Driver stacktrace:\n",
      "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2041)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2029)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2028)\n",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2028)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:966)\n",
      "  at scala.Option.foreach(Option.scala:257)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:966)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2262)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2211)\n",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2200)\n",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:777)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:401)\n",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:84)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:165)\n",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:74)\n",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:751)\n",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:710)\n",
      "  ... 64 elided\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$10: (double) => string)\n",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:291)\n",
      "  at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:283)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:836)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "  ... 3 more\n",
      "Caused by: org.apache.spark.SparkException: Unseen index: 1168.0 ??\n",
      "  at org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:404)\n",
      "  at org.apache.spark.ml.feature.IndexToString$$anonfun$10.apply(StringIndexer.scala:399)\n",
      "  ... 21 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"predictionLabel\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f7edc32434dc0bff180ce1d4c6296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a06798eca4b7\n"
     ]
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"indexedLabel\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

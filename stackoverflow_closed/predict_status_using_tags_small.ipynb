{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.{NaiveBayes,NaiveBayesModel,RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column in the dataset that causes Spark's CSV read method to incorrectly parse the dataset.  The `BodyMarkdown` field includes values that span multiple lines.  Spark treats the line breaks within the values as new rows in the dataset.  We must specify the additional options `quote`, `escape`, and `multiLine` to read the data set properly.  See this [blog post](https://kokes.github.io/blog/2018/05/19/spark-sane-csv-processing.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customSchema = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
       "df = [PostId: double, PostCre...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCre..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"train-sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 140,000 million observations across 15 different columns.  However, identifiers such as `PostId` and `OwnerUserId` will be discarded.  `OpenStatus` and `PostClosedDate` will also be discarded as features since the former is our response variable and the latter implies the response value.  That leaves 11 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "|     PostId|   PostCreationDate|OwnerUserId|  OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|               Title|        BodyMarkdown|      Tag1|           Tag2| Tag3|Tag4|Tag5|     PostClosedDate|    OpenStatus|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OpenStatus` column seems to have an issue with trailing whitespace -- there should be a column ending pipe operator `|` in the printed table above.  Using `trim` didn't work, so we're taking a regex approach instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"OpenStatus\", regexp_extract(col(\"OpenStatus\"), \"([\\\\w\\\\s]+\\\\w)\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the percentage of null values for each column, we see that `Tag1` is rarely missing but that the other `TagX` fields increase in sparsity as `X` increases.  `Tag4` and `Tag5` will likely need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PostId,0.0)\n",
      "(PostCreationDate,0.0)\n",
      "(OwnerUserId,0.0)\n",
      "(OwnerCreationDate,0.0)\n",
      "(ReputationAtPostCreation,0.0)\n",
      "(OwnerUndeletedAnswerCountAtPostTime,0.0)\n",
      "(Title,0.0)\n",
      "(BodyMarkdown,0.0)\n",
      "(Tag1,7.129006501653929E-6)\n",
      "(Tag2,0.19408720200752824)\n",
      "(Tag3,0.45855195619938405)\n",
      "(Tag4,0.7172208281053952)\n",
      "(Tag5,0.8879534048135052)\n",
      "(PostClosedDate,0.5)\n",
      "(OpenStatus,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nullCountDf = [PostId: double, PostCreationDate: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: double ... 13 more fields]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nullCountDf = df.\n",
    "    select(df.columns.map(c => (sum(when(col(c).isNull || col(c) === \"\" || col(c).isNaN, 1).otherwise(0)) / df.count()).alias(c)): _*)\n",
    "\n",
    "nullCountDf.\n",
    "    columns.\n",
    "    zip(nullCountDf.collect()(0).toSeq).\n",
    "    foreach(tuple => println(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the `Tag` fields with \"unknown\" so they can be processed as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.fill(\"unknown\", Seq(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - frequency of `OpenStatus` and `TagX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_simple = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the label `OpenStatus` has only 5 distinct values while each of the tags has over 5,000 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Tag1 values: 5212\n",
      "Distinct Tag2 values: 9295\n",
      "Distinct Tag3 values: 11083\n",
      "Distinct Tag4 values: 10030\n",
      "Distinct Tag5 values: 7607\n",
      "Distinct OpenStatus values: 5\n"
     ]
    }
   ],
   "source": [
    "df_simple.\n",
    "    columns.\n",
    "    map(c => {\n",
    "            val count = df_simple.select(c).distinct.count\n",
    "            f\"Distinct $c values: $count\"\n",
    "    }).\n",
    "    foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OpenStatus` takes on the value `open` in 50% of all cases.  `too localized` appears in the least number of cases at 4.4%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|         OpenStatus|count|     perc_of_total|\n",
      "+-------------------+-----+------------------+\n",
      "|               open|70136|              50.0|\n",
      "|not a real question|30789|21.949498117942284|\n",
      "|          off topic|17530|12.497148397399338|\n",
      "|   not constructive|15659|11.163311280939888|\n",
      "|      too localized| 6158| 4.390042203718489|\n",
      "+-------------------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status_counts = [OpenStatus: string, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OpenStatus: string, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val status_counts = df_simple.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    sort(col(\"count\").desc).\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / df_simple.count())\n",
    "\n",
    "status_counts.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cut down on the number of distinct `TagX` values, we'll try keeping the top 90% of observations.  The bottom 10% will be replaced with \"other\".  Below we count the number of distinct tags in the top 90% of observations for each `TagX` column.  `Tag2`, `Tag3`, and `Tag4` still have over 1800 distinct values, but this is far less than the 10,000+ values they originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_percentile_count: (df: org.apache.spark.sql.DataFrame, percentile: Double)(col_name: String)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(382.0, 1855.0, 2644.0, 1861.0, 126.0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_percentile_count(df: DataFrame, percentile: Double)(col_name: String): Double = {\n",
    "    \n",
    "    val cumsum_window = Window.\n",
    "      orderBy(col(\"count\").desc).\n",
    "      rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    val total_window = Window.\n",
    "        rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df.\n",
    "        groupBy(col_name).\n",
    "        count().\n",
    "        orderBy(col(\"count\").desc).\n",
    "        withColumn(\"fracObs\", sum(col(\"count\")).over(cumsum_window) / sum(col(\"count\")).over(total_window)).\n",
    "        filter(col(\"fracObs\") <= percentile).\n",
    "        count()\n",
    "}\n",
    "\n",
    "Array(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\").\n",
    "    map(get_percentile_count(df_simple, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: NaiveBayes with Tags as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll take a simple approach - encode the Tag columns as categorical variables and predict `OpenStatus` using Naive Bayes. Spark's RF Classifier example will serve as a reference ([link](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_simple = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Reducing `TagX` Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we'll be replacing the bottom 10\\% of `TagX` values with \"else\".  We need to avoid any data leaking from the test set to the training set, and we also need to apply the same replacement rules to both sets.  The below function `replaceInfrequentVals` accomplishes this.  It takes the training and test sets, along with a column name and percentile cutoff.  It then calls `getPercentileLookup` to create a lookup table of the specified columns' values in the training set alongside their percentile rank in terms of frequency.  This lookup table is then joined to both the training and test sets; column values with a percentile rank greater than the provided cutoff are set to \"else\".  The function `replaceMultipleInfrequentVals` successively applies `replaceInfrequentVals` for an array of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getPercentileLookup: (df: org.apache.spark.sql.DataFrame, colName: String)org.apache.spark.sql.DataFrame\n",
       "replaceInfrequentVals: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, colName: String, percentileCutoff: Double)List[org.apache.spark.sql.DataFrame]\n",
       "replaceMultipleInfrequentVals: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, colNames: Array[String], percentileCutoff: Double)List[org.apache.spark.sql.DataFrame]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * Get a lookup table of column values and their percentile rank for frequency.\n",
    " *\n",
    " * @param  df        A DataFrame which includes a column specified by colName.\n",
    " * @param  colName   The name of the column for which to calculate percentile ranks.\n",
    " */\n",
    "def getPercentileLookup(df: DataFrame, colName: String): DataFrame = {\n",
    "    \n",
    "    val cumsumWindow = Window.\n",
    "      orderBy(col(\"count\").desc).\n",
    "      rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    val totalWindow = Window.\n",
    "        rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df.\n",
    "        groupBy(colName).\n",
    "        count().\n",
    "        orderBy(col(\"count\").desc).\n",
    "        withColumn(\"percentileRank\", sum(col(\"count\")).over(cumsumWindow) / sum(col(\"count\")).over(totalWindow)).\n",
    "        drop(\"count\")\n",
    "    \n",
    "}\n",
    "\n",
    "/**\n",
    " * Replace column values that appear infrequently in the training set with \"else\"\n",
    " * in both the training and test sets. \n",
    " *\n",
    " * @param  train             DataFrame for which to calculate the frequency perentile rank.\n",
    " * @param  test              Additional DataFrame for which to replace infrequent values.\n",
    " * @param  colName           Column for which to replace infrequent values.\n",
    " * @param  percentileCutoff  Values with percentile rank higher than this number are replaced by \"else\".\n",
    " * @return A modified train and test DataFrame.\n",
    " */\n",
    "def replaceInfrequentVals(\n",
    "    train: DataFrame, \n",
    "    test: DataFrame, \n",
    "    colName: String,\n",
    "    percentileCutoff: Double\n",
    "): List[DataFrame] = {\n",
    "    \n",
    "    val percentileLookup = getPercentileLookup(train, colName)\n",
    "    \n",
    "    val trainReplaced = train.\n",
    "        join(percentileLookup, Seq(colName), \"left_outer\").\n",
    "        na.fill(1, Seq(\"percentileRank\")).\n",
    "        withColumn(colName, when(col(\"percentileRank\") <= percentileCutoff, col(colName)).otherwise(\"else\")).\n",
    "        drop(\"percentileRank\")\n",
    "    \n",
    "    val testReplaced = test.\n",
    "        join(percentileLookup, Seq(colName), \"left_outer\").\n",
    "        na.fill(1, Seq(\"percentileRank\")).\n",
    "        withColumn(colName, when(col(\"percentileRank\") <= percentileCutoff, col(colName)).otherwise(\"else\")).\n",
    "        drop(\"percentileRank\")\n",
    "    \n",
    "    List(trainReplaced, testReplaced)\n",
    "    \n",
    "}\n",
    "\n",
    "/**\n",
    " * Apply `replaceInfrequentVals` to multiple columns.\n",
    " */\n",
    "def replaceMultipleInfrequentVals(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    colNames: Array[String],\n",
    "    percentileCutoff: Double\n",
    "): List[DataFrame] = {\n",
    "    \n",
    "    var trainOut = train\n",
    "    var testOut  = test\n",
    "    \n",
    "    colNames.foreach(x => {\n",
    "        var dfs = replaceInfrequentVals(trainOut, testOut, x, percentileCutoff)\n",
    "        trainOut = dfs(0)\n",
    "        testOut  = dfs(1)\n",
    "    })\n",
    "    \n",
    "    List(trainOut, testOut)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a StringIndexer for each Tag column; rather than create 5 variables we'll take a functional approach.  Note that `setHandleInvalid` is set to \"keep\" so that the indexer adds new indexes when it sees new labels in data sets other than our current data set ([StackOverflow link](https://stackoverflow.com/a/43917703/11407644))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(Tag1, Tag2, Tag3)\n",
       "featureIndexers = Array(strIdx_a2bd6098cbc5, strIdx_0ebb50a8f750, strIdx_9f785ff0df21)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(strIdx_a2bd6098cbc5, strIdx_0ebb50a8f750, strIdx_9f785ff0df21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = Array[String](\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\")\n",
    "\n",
    "val featureIndexers = featureCols.map { colName =>\n",
    "    new StringIndexer().\n",
    "        setInputCol(colName).\n",
    "        setOutputCol(\"indexed\" + colName).\n",
    "        setHandleInvalid(\"keep\").\n",
    "        fit(df_simple)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML models expect a feature vector to be the only predictor.  [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is a transformer that combines a list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_7d60450d8a13\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_7d60450d8a13"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureCols.map{x => \"indexed\" + x}).\n",
    "    setOutputCol(\"features\").\n",
    "    setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the categorical features, we index the response.  Keeping with convention, the indexed response is called `indexedLabel` rather than `indexedOpenStatus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_941d13fcfb98\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "strIdx_941d13fcfb98"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().\n",
    "    setInputCol(\"OpenStatus\").\n",
    "    setOutputCol(\"indexedLabel\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(df_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the response is indexed, we need a way to transform the predicted response back to its original string value.  This inverse transformer is called [`IndexToString`](https://spark.apache.org/docs/latest/ml-features.html#indextostring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelConverter = idxToStr_504786612218\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idxToStr_504786612218"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelConverter = new IndexToString().\n",
    "    setInputCol(\"prediction\").\n",
    "    setOutputCol(\"predictionLabel\").\n",
    "    setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can specify our model, a [`NaiveBayes`](https://spark.apache.org/docs/latest/ml-classification-regression.html#naive-bayes) classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb = nb_2ea76f312034\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "nb_2ea76f312034"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb = new NaiveBayes().\n",
    "    setLabelCol(\"indexedLabel\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setSmoothing(1.0).\n",
    "    setModelType(\"multinomial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nb_pipeline = pipeline_0a22e2e1f953\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_0a22e2e1f953"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nb_pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, nb, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [Tag1: string, Tag2: string ... 2 more fields]\n",
       "test = [Tag1: string, Tag2: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 2 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = df_simple.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce distinct occurrences of `TagX` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainReduced = [Tag3: string, Tag2: string ... 2 more fields]\n",
       "testReduced = [Tag3: string, Tag2: string ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag3: string, Tag2: string ... 2 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val List(trainReduced, testReduced) = replaceMultipleInfrequentVals(train,\n",
    "                                                                    test,\n",
    "                                                                    Array(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"),\n",
    "                                                                    0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to the training set to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model = pipeline_0a22e2e1f953\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_0a22e2e1f953"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = nb_pipeline.fit(trainReduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-------------------+\n",
      "|predictionLabel|          OpenStatus|           features|\n",
      "+---------------+--------------------+-------------------+\n",
      "|[3.0,1522.0,2433.0]|\n",
      "|  [2.0,10.0,2433.0]|\n",
      "|  [2.0,14.0,2433.0]|\n",
      "|  [3.0,58.0,2433.0]|\n",
      "|   [4.0,13.0,723.0]|\n",
      "+---------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions = [Tag3: string, Tag2: string ... 11 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag3: string, Tag2: string ... 11 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model.transform(testReduced)\n",
    "predictions.select(\"predictionLabel\", \"OpenStatus\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up [MulticlassClassificationEvaluator](https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_3dd72513707f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_3dd72513707f"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"indexedLabel\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy was ~15%.  To get a sense for how good or bad this is, let's look at how well the model would have performed if it simply guessed the most common response (`Open`) each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy = 0.15867281456481439\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.15867281456481439"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|          OpenStatus|count|     perc_of_total|\n",
      "+--------------------+-----+------------------+\n",
      "| 1896|4.5063459618766935|\n",
      "|21120|50.197271474069495|\n",
      "| 9176| 21.80919332604459|\n",
      "| 4629| 11.00204401768313|\n",
      "| 5253|12.485145220326093|\n",
      "+--------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / lit(predictions.count())).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model had guessed `open` for each test observation, the accuracy would have been ~50% -- much higher than the ~15% that NaiveBayes achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at class-specific performance, we see that the model was best at predicting `` and worst at predicting ``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          OpenStatus|           accurracy|\n",
      "+--------------------+--------------------+\n",
      "|  0.4868143459915612|\n",
      "| 0.18792613636363636|\n",
      "|0.001525719267654...|\n",
      "|0.002592352559948153|\n",
      "| 0.33466590519703027|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    select(\"predictionLabel\", \"OpenStatus\").\n",
    "    withColumn(\"success\", when(col(\"predictionLabel\") === col(\"OpenStatus\"), 1).otherwise(0)).\n",
    "    groupBy(\"OpenStatus\").\n",
    "    agg((sum(col(\"success\")) / count(\"*\")).alias(\"accurracy\")).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully implemented a Spark NLP pipeline on a small subset of our full training set.  This pipeline involved..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

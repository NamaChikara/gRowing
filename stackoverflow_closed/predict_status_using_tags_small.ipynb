{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary spark functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._ \n",
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.classification.{NaiveBayes,NaiveBayesModel,RandomForestClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.ml._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.host,c4dc8eff627e)\n",
      "(spark.driver.port,42179)\n",
      "(spark.repl.class.uri,spark://c4dc8eff627e:42179/classes)\n",
      "(spark.jars,file:/opt/conda/share/jupyter/kernels/apache_toree_scala/lib/toree-assembly-0.3.0-incubating.jar)\n",
      "(spark.repl.class.outputDir,/tmp/spark-f7341c0c-b01e-4825-86ae-548b96318d54/repl-ca17b19e-425b-4c39-957c-e2fb0329ac37)\n",
      "(spark.app.name,Apache Toree)\n",
      "(spark.executor.id,driver)\n",
      "(spark.driver.extraJavaOptions,-Dlog4j.logLevel=info)\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.master,local[*])\n",
      "(spark.app.id,local-1589739451597)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.getAll.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kokes.github.io/blog/2018/05/19/spark-sane-csv-processing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample of training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a column in the dataset that causes Spark's CSV read method to incorrectly parse the dataset.  The `BodyMarkdown` field includes values that span multiple lines.  Spark treats the line breaks within the values as new rows in the dataset.  We must specify the additional options `quote`, `escape`, and `multiLine` to read the data set properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "customSchema = StructType(StructField(PostId,DoubleType,true), StructField(PostCreationDate,StringType,true), StructField(OwnerUserId,DoubleType,true), StructField(OwnerCreationDate,StringType,true), StructField(ReputationAtPostCreation,DoubleType,true), StructField(OwnerUndeletedAnswerCountAtPostTime,DoubleType,true), StructField(Title,StringType,true), StructField(BodyMarkdown,StringType,true), StructField(Tag1,StringType,true), StructField(Tag2,StringType,true), StructField(Tag3,StringType,true), StructField(Tag4,StringType,true), StructField(Tag5,StringType,true), StructField(PostClosedDate,StringType,true), StructField(OpenStatus,StringType,true))\n",
       "df = [PostId: double, PostCre...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCre..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val customSchema = StructType(Array(\n",
    "    StructField(\"PostId\", DoubleType, true),\n",
    "    StructField(\"PostCreationDate\", StringType, true),\n",
    "    StructField(\"OwnerUserId\", DoubleType, true),\n",
    "    StructField(\"OwnerCreationDate\", StringType, true),\n",
    "    StructField(\"ReputationAtPostCreation\", DoubleType, true),\n",
    "    StructField(\"OwnerUndeletedAnswerCountAtPostTime\", DoubleType, true),\n",
    "    StructField(\"Title\", StringType, true),\n",
    "    StructField(\"BodyMarkdown\", StringType, true),\n",
    "    StructField(\"Tag1\", StringType, true),\n",
    "    StructField(\"Tag2\", StringType, true),\n",
    "    StructField(\"Tag3\", StringType, true),\n",
    "    StructField(\"Tag4\", StringType, true),\n",
    "    StructField(\"Tag5\", StringType, true),\n",
    "    StructField(\"PostClosedDate\", StringType, true),\n",
    "    StructField(\"OpenStatus\", StringType, true)\n",
    "))\n",
    "\n",
    "var df = spark.\n",
    "    read.\n",
    "    option(\"quote\", \"\\\"\").\n",
    "    option(\"escape\", \"\\\"\").\n",
    "    option(\"multiLine\", \"true\").\n",
    "    option(\"header\", \"true\").\n",
    "    schema(customSchema).\n",
    "    csv(\"train-sample.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 140,000 million observations across 15 different columns.  However, identifiers such as `PostId` and `OwnerUserId` will be discarded.  `OpenStatus` and `PostClosedDate` will also be discarded as features since the former is our response variable and the latter implies the response value.  That leaves 11 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140272"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "|     PostId|   PostCreationDate|OwnerUserId|  OwnerCreationDate|ReputationAtPostCreation|OwnerUndeletedAnswerCountAtPostTime|               Title|        BodyMarkdown|      Tag1|           Tag2| Tag3|Tag4|Tag5|     PostClosedDate|    OpenStatus|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "|\n",
      "+-----------+-------------------+-----------+-------------------+------------------------+-----------------------------------+--------------------+--------------------+----------+---------------+-----+----+----+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `OpenStatus` column seems to have an issue with trailing whitespace -- there should be a column ending pipe operator `|` in the printed table above.  Using `trim` didn't work, so we're taking a regex approach instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumn(\"OpenStatus\", regexp_extract(col(\"OpenStatus\"), \"([\\\\w\\\\s]+\\\\w)\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the percentage of null values for each column, we see that `Tag1` is rarely missing but that the other `TagX` fields increase in sparsity as `X` increases.  `Tag4` and `Tag5` will likely need to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PostId,0.0)\n",
      "(PostCreationDate,0.0)\n",
      "(OwnerUserId,0.0)\n",
      "(OwnerCreationDate,0.0)\n",
      "(ReputationAtPostCreation,0.0)\n",
      "(OwnerUndeletedAnswerCountAtPostTime,0.0)\n",
      "(Title,0.0)\n",
      "(BodyMarkdown,0.0)\n",
      "(Tag1,7.129006501653929E-6)\n",
      "(Tag2,0.19408720200752824)\n",
      "(Tag3,0.45855195619938405)\n",
      "(Tag4,0.7172208281053952)\n",
      "(Tag5,0.8879534048135052)\n",
      "(PostClosedDate,0.5)\n",
      "(OpenStatus,0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nullCountDf = [PostId: double, PostCreationDate: double ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: double ... 13 more fields]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nullCountDf = df.\n",
    "    select(df.columns.map(c => (sum(when(col(c).isNull || col(c) === \"\" || col(c).isNaN, 1).otherwise(0)) / df.count()).alias(c)): _*)\n",
    "\n",
    "nullCountDf.\n",
    "    columns.\n",
    "    zip(nullCountDf.collect()(0).toSeq).\n",
    "    foreach(tuple => println(tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the `Tag` fields with \"unknown\" so they can be processed as categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [PostId: double, PostCreationDate: string ... 13 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[PostId: double, PostCreationDate: string ... 13 more fields]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.na.fill(\"unknown\", Seq(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA - frequency of `OpenStatus` and `TagX`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_simple = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the label `OpenStatus` has only 5 distinct values while each of the tags has over 5,000 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Tag1 values: 5212\n",
      "Distinct Tag2 values: 9295\n",
      "Distinct Tag3 values: 11083\n",
      "Distinct Tag4 values: 10030\n",
      "Distinct Tag5 values: 7607\n",
      "Distinct OpenStatus values: 5\n"
     ]
    }
   ],
   "source": [
    "df_simple.\n",
    "    columns.\n",
    "    map(c => {\n",
    "            val count = df_simple.select(c).distinct.count\n",
    "            f\"Distinct $c values: $count\"\n",
    "    }).\n",
    "    foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`OpenStatus` takes on the value `open` in 50% of all cases.  `too localized` appears in the least number of cases at 4.4%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|         OpenStatus|count|     perc_of_total|\n",
      "+-------------------+-----+------------------+\n",
      "|               open|70136|              50.0|\n",
      "|not a real question|30789|21.949498117942284|\n",
      "|          off topic|17530|12.497148397399338|\n",
      "|   not constructive|15659|11.163311280939888|\n",
      "|      too localized| 6158| 4.390042203718489|\n",
      "+-------------------+-----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "status_counts = [OpenStatus: string, count: bigint ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[OpenStatus: string, count: bigint ... 1 more field]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val status_counts = df_simple.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    sort(col(\"count\").desc).\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / df_simple.count())\n",
    "\n",
    "status_counts.show(numRows = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cut down on the number of distinct `TagX` values, we'll try keeping the top 90% of observations.  The bottom 10% will be replaced with \"other\".  Below we count the number of distinct tags in the top 90% of observations for each `TagX` column.  `Tag2`, `Tag3`, and `Tag4` still have over 1800 distinct values, but this is far less than the 10,000+ values they originally had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_percentile_count: (df: org.apache.spark.sql.DataFrame, percentile: Double)(col_name: String)Double\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(382.0, 1855.0, 2644.0, 1861.0, 126.0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_percentile_count(df: DataFrame, percentile: Double)(col_name: String): Double = {\n",
    "    \n",
    "    val cumsum_window = Window.\n",
    "      orderBy(col(\"count\").desc).\n",
    "      rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    val total_window = Window.\n",
    "        rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df.\n",
    "        groupBy(col_name).\n",
    "        count().\n",
    "        orderBy(col(\"count\").desc).\n",
    "        withColumn(\"fracObs\", sum(col(\"count\")).over(cumsum_window) / sum(col(\"count\")).over(total_window)).\n",
    "        filter(col(\"fracObs\") <= percentile).\n",
    "        count()\n",
    "}\n",
    "\n",
    "Array(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\").\n",
    "    map(get_percentile_count(df_simple, 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple model: Random Forest with Tags as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we'll take a simple approach - encode the Tag columns as categorical variables and predict `OpenStatus` using Random Forest. Spark's RF Classifier example will serve as a reference ([link](https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_simple = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_simple = df.\n",
    "    select(col(\"Tag1\"), col(\"Tag2\"), col(\"Tag3\"), col(\"Tag4\"), col(\"Tag5\"), col(\"OpenStatus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for Reducing `TagX` Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we'll be replacing the bottom 10\\% of `TagX` values with \"else\".  We need to avoid any data leaking from the test set to the training set, and we also need to apply the same replacement rules to both sets.  The below function `replaceInfrequentVals` accomplishes this.  It takes the training and test sets, along with a column name and percentile cutoff.  It then calls `getPercentileLookup` to create a lookup table of the specified columns' values in the training set alongside their percentile rank in terms of frequency.  This lookup table is then joined to both the training and test sets; column values with a percentile rank greater than the provided cutoff are set to \"else\".  The function `replaceMultipleInfrequentVals` successively applies `replaceInfrequentVals` for an array of column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getPercentileLookup: (df: org.apache.spark.sql.DataFrame, colName: String)org.apache.spark.sql.DataFrame\n",
       "replaceInfrequentVals: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, colName: String, percentileCutoff: Double)List[org.apache.spark.sql.DataFrame]\n",
       "replaceMultipleInfrequentVals: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, colNames: Array[String], percentileCutoff: Double)List[org.apache.spark.sql.DataFrame]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * Get a lookup table of column values and their percentile rank for frequency.\n",
    " *\n",
    " * @param  df        A DataFrame which includes a column specified by colName.\n",
    " * @param  colName   The name of the column for which to calculate percentile ranks.\n",
    " */\n",
    "def getPercentileLookup(df: DataFrame, colName: String): DataFrame = {\n",
    "    \n",
    "    val cumsumWindow = Window.\n",
    "      orderBy(col(\"count\").desc).\n",
    "      rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "    val totalWindow = Window.\n",
    "        rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    df.\n",
    "        groupBy(colName).\n",
    "        count().\n",
    "        orderBy(col(\"count\").desc).\n",
    "        withColumn(\"percentileRank\", sum(col(\"count\")).over(cumsumWindow) / sum(col(\"count\")).over(totalWindow)).\n",
    "        drop(\"count\")\n",
    "    \n",
    "}\n",
    "\n",
    "def replaceInfrequentVals(\n",
    "    train: DataFrame, \n",
    "    test: DataFrame, \n",
    "    colName: String,\n",
    "    percentileCutoff: Double\n",
    "): List[DataFrame] = {\n",
    "    \n",
    "    val percentileLookup = getPercentileLookup(train, colName)\n",
    "    \n",
    "    val trainReplaced = train.\n",
    "        join(percentileLookup, Seq(colName), \"left_outer\").\n",
    "        na.fill(1, Seq(\"percentileRank\")).\n",
    "        withColumn(colName, when(col(\"percentileRank\") <= percentileCutoff, col(colName)).otherwise(\"else\")).\n",
    "        drop(\"percentileRank\")\n",
    "    \n",
    "    val testReplaced = test.\n",
    "        join(percentileLookup, Seq(colName), \"left_outer\").\n",
    "        na.fill(1, Seq(\"percentileRank\")).\n",
    "        withColumn(colName, when(col(\"percentileRank\") <= percentileCutoff, col(colName)).otherwise(\"else\")).\n",
    "        drop(\"percentileRank\")\n",
    "    \n",
    "    List(trainReplaced, testReplaced)\n",
    "    \n",
    "}\n",
    "\n",
    "def replaceMultipleInfrequentVals(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    colNames: Array[String],\n",
    "    percentileCutoff: Double\n",
    "): List[DataFrame] = {\n",
    "    \n",
    "    var trainOut = train\n",
    "    var testOut  = test\n",
    "    \n",
    "    colNames.foreach(x => {\n",
    "        var dfs = replaceInfrequentVals(trainOut, testOut, x, percentileCutoff)\n",
    "        trainOut = dfs(0)\n",
    "        testOut  = dfs(1)\n",
    "    })\n",
    "    \n",
    "    List(trainOut, testOut)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline for Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a StringIndexer for each Tag column; rather than create 5 variables we'll take a functional approach.  Note that `setHandleInvalid` is set to \"keep\" so that the indexer adds new indexes when it sees new labels in data sets other than our current data set ([StackOverflow link](https://stackoverflow.com/a/43917703/11407644))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "featureCols = Array(Tag1, Tag2, Tag3, Tag4, Tag5)\n",
       "featureIndexers = Array(strIdx_cd35bdb9defe, strIdx_a95193309b23, strIdx_33e33834805e, strIdx_7999a26db07b, strIdx_d98b05ca30cc)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(strIdx_cd35bdb9defe, strIdx_a95193309b23, strIdx_33e33834805e, strIdx_7999a26db07b, strIdx_d98b05ca30cc)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val featureCols = Array[String](\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\")\n",
    "\n",
    "val featureIndexers = featureCols.map { colName =>\n",
    "    new StringIndexer().\n",
    "        setInputCol(colName).\n",
    "        setOutputCol(\"indexed\" + colName).\n",
    "        setHandleInvalid(\"keep\").\n",
    "        fit(df_simple)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark ML models expect a feature vector to be the only predictor.  [`VectorAssembler`](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler) is a transformer that combines a list of columns into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler = vecAssembler_7c240f0bbf94\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vecAssembler_7c240f0bbf94"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().\n",
    "    setInputCols(featureCols.map{x => \"indexed\" + x}).\n",
    "    setOutputCol(\"features\").\n",
    "    setHandleInvalid(\"keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the categorical features, we index the response.  Keeping with convention, the indexed response is called `indexedLabel` rather than `indexedOpenStatus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelIndexer = strIdx_984ba310d575\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "strIdx_984ba310d575"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().\n",
    "    setInputCol(\"OpenStatus\").\n",
    "    setOutputCol(\"indexedLabel\").\n",
    "    setHandleInvalid(\"keep\").\n",
    "    fit(df_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the response is indexed, we need a way to transform the predicted response back to its original string value.  This inverse transformer is called [`IndexToString`](https://spark.apache.org/docs/latest/ml-features.html#indextostring):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labelConverter = idxToStr_6c307e7f3f0f\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "idxToStr_6c307e7f3f0f"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelConverter = new IndexToString().\n",
    "    setInputCol(\"prediction\").\n",
    "    setOutputCol(\"predictionLabel\").\n",
    "    setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can specify our model, a [`RandomForestClassifer`](https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf = rfc_ea0c7d113d8c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "rfc_ea0c7d113d8c"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier().\n",
    "    setLabelCol(\"indexedLabel\").\n",
    "    setFeaturesCol(\"features\").\n",
    "    setNumTrees(10).\n",
    "    setMaxBins(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf_pipeline = pipeline_03623729e54c\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pipeline_03623729e54c"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf_pipeline = new Pipeline().\n",
    "    setStages(featureIndexers ++ Array(assembler, labelIndexer, rf, labelConverter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [Tag1: string, Tag2: string ... 4 more fields]\n",
       "test = [Tag1: string, Tag2: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag1: string, Tag2: string ... 4 more fields]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = df_simple.randomSplit(Array(0.7, 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce distinct occurrences of `TagX` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainReduced = [Tag5: string, Tag4: string ... 4 more fields]\n",
       "testReduced = [Tag5: string, Tag4: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Tag5: string, Tag4: string ... 4 more fields]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val List(trainReduced, testReduced) = replaceMultipleInfrequentVals(train,\n",
    "                                                                    test,\n",
    "                                                                    Array(\"Tag1\", \"Tag2\", \"Tag3\", \"Tag4\", \"Tag5\"),\n",
    "                                                                    0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the pipeline to the training set to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in thread \"Executor task launch worker for task 28178\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:147)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:274)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:70)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:618)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Exception in thread \"Executor task launch worker for task 28179\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:147)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:274)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:70)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:618)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Exception in thread \"Executor task launch worker for task 28181\" java.lang.SecurityException: Not allowed to invoke System.exit!\n",
      "\tat org.apache.toree.security.KernelSecurityManager.checkExit(KernelSecurityManager.scala:147)\n",
      "\tat java.lang.Runtime.halt(Runtime.java:274)\n",
      "\tat org.apache.spark.util.SparkUncaughtExceptionHandler.uncaughtException(SparkUncaughtExceptionHandler.scala:70)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:618)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 1 in stage 815.0 failed 1 times, most recent failure: Lost task 1.0 in stage 815.0 (TID 28178, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
       "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
       "\tat scala.Array$.tabulate(Array.scala:331)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: \tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
       "\tat scala.Array$.tabulate(Array.scala:331)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
       "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
       "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:748)\n",
       "Driver stacktrace:\n",
       "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
       "  at org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:142)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier$$anonfun$train$1.apply(RandomForestClassifier.scala:120)\n",
       "  at org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
       "  at scala.util.Try$.apply(Try.scala:192)\n",
       "  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:120)\n",
       "  at org.apache.spark.ml.classification.RandomForestClassifier.train(RandomForestClassifier.scala:46)\n",
       "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
       "  at org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
       "  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n",
       "  at org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n",
       "  at scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
       "  at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
       "  at scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n",
       "  at scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n",
       "  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n",
       "  ... 52 elided\n",
       "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
       "  at org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
       "  at scala.Array$.tabulate(Array.scala:331)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
       "  at org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
       "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
       "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
       "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
       "  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
       "  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
       "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
       "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = rf_pipeline.fit(trainReduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val predictions = model.transform(testReduced)\n",
    "predictions.select(\"predictionLabel\", \"OpenStatus\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up [MulticlassClassificationEvaluator](https://spark.apache.org/docs/2.3.0/api/scala/index.html#org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_28b51846e9f0\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mcEval_28b51846e9f0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "    .setLabelCol(\"label\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "    .setMetricName(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy was ~27%.  To get a sense for how good or bad this is, let's look at how well the model would have performed if it simply guessed the most common response (`Open`) each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy = 0.27433144919963903\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.27433144919963903"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+------------------+\n",
      "|         OpenStatus|count|     perc_of_total|\n",
      "+-------------------+-----+------------------+\n",
      "|not a real question| 9334|22.167862062413906|\n",
      "|      too localized| 1842| 4.374673443214744|\n",
      "|   not constructive| 4692|11.143304992162637|\n",
      "|          off topic| 5274|12.525530803210943|\n",
      "|               open|20964| 49.78862869899777|\n",
      "+-------------------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    groupBy(\"OpenStatus\").\n",
    "    count().\n",
    "    withColumn(\"perc_of_total\", lit(100) * col(\"count\") / lit(predictions.count())).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model had guessed `open` for each test observation, the accuracy would have been 49.8% -- much higher than the 27% that NaiveBayes achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|         OpenStatus|           accurracy|\n",
      "+-------------------+--------------------+\n",
      "|not a real question|0.030319262909792158|\n",
      "|      too localized| 0.05863192182410423|\n",
      "|   not constructive|0.017263427109974423|\n",
      "|          off topic|   0.586841107318923|\n",
      "|               open|  0.3808433505056287|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.\n",
    "    select(\"predictionLabel\", \"OpenStatus\").\n",
    "    withColumn(\"success\", when(col(\"predictionLabel\") === col(\"OpenStatus\"), 1).otherwise(0)).\n",
    "    groupBy(\"OpenStatus\").\n",
    "    agg((sum(col(\"success\")) / count(\"*\")).alias(\"accurracy\")).\n",
    "    show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A look at class-specific performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

---
title: "exploratory_analysis"
author: "ZackBarry"
date: "1/5/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DataExplorer)
library(ggpubr)
library(leaflet)
library(leaflet.extras)
library(sqldf)

dataset <- read_csv("Data/AB_NYC_2019.csv")
```

## Abstract



## Data source

This data set has nearly 50,000 observations of 16 different variables, 6 of 
which are discrete and 10 of which are continuous.

## Acquiring/Loading Data

```{r}
dataset <- read_csv("Data/AB_NYC_2019.csv")
head(dataset)
```

```{r}
# print size of data set
dim(dataset)
```

```{r}
# high-level overview of data types and missing data
plot_intro(dataset)
```

A high-level overview of the types of variables / amount of missing data shows 
that we have 10 continuous and 6 discrete variables.  Most rows have no missing
values, and the total number of missing observations is quite low.  Since
there are 50,000 observations to draw from, we should be able to generate some
great insights!

## Data Wrangling

```{r}
# check which data is missing
plot_missing(dataset)
```

```{r}
# further investigate missing data
dataset %>%
  filter(is.na(reviews_per_month) | is.na(last_review)) %>%
  count(number_of_reviews)
```

Missing observations are mostly limited to `reviews_per_month` and `last_review`.
Upon further investigation, these missing observations occur when `number_of_reviews`
is equal to 0.  This makes sense because if a property has never received any
reviews there is no concept of "most recent review" nor can a rate of reviews be
calculated.  However, it makes sense to set `reviews_per_month` to 0 when it is
missing.  For `last_review`, we'll replace it with a new variable 
`days_since_review`.  Each non-missing observation is formatted YYYY-MM-DD 
so this will be easy to do.  Furthermore we can replace the missing values with
-1 so that the new variable is prepped for modeling.

```{r}
# handle missing `reviews_per_month`
dataset <- dataset %>%
  mutate(reviews_per_month = replace_na(reviews_per_month, 0))
```

```{r}
# handle missing `last_review` by creating `days_since_review` with -1 standing
# in for missing values
dataset <- dataset %>%
  mutate(
    last_review = as.Date(last_review, "%Y-%m-%d"),
    latest_review_day = max(last_review, na.rm = TRUE)
  ) %>%
  mutate(days_since_review = as.numeric(difftime(latest_review_day, last_review, units = "days")) + 1) %>%
  mutate(days_since_review = replace_na(days_since_review, -1)) %>%
  select(-c(last_review, latest_review_day))
```

The other missing observations are in the `host_name` and `name` variables.  
Since there is already a `host_id` field, these are not needed for identifying
distinct hosts.  If we were to use these variables for modeling purposes, we
could replace their missing values with `host_id`.  However, using `name` opens
up privacy and discrimination concerns that we should avoid.
```{r}
# drop host_name and name as protected information
dataset <- select(dataset, -c(name, host_name))
```

## Exploring and visualizating data.

The bar plots below give further insight into our discrete (categorical)
variables.  Our `neighbourhood_group` observations are limited to 5 neighborhood 
groups, with Manhattan and Brooklyn comprising the majority.  Room type is mostly 
evenly split between "entire home/apt" and "private room", with "shared room" 
rarely appearing.  Unsurprisingly, `days_since_review` has too many distinct 
categories to plot - it should probably be treated as a continuous variable.
Finally, there are 221 distinct neighborhoods within our neighborhood groups.
```{r}
DataExplorer::plot_bar(dataset)
```

```{r}
# neighborhood-specific room_type occurences
dataset %>%
  count(neighbourhood_group, room_type) %>%
  arrange(room_type, desc(n))
```

```{r}
# count of distinct neighbourhoods per group
dataset %>%
  distinct(neighbourhood_group, neighbourhood) %>%
  count(neighbourhood_group)
```


Let's further investigate our continuous variables via histograms:
```{r}
plot_histogram(dataset)
```

We see that `id` is fairly evenly distributed, while most other metrics are
right-skewed.  `latidue` and `longitude` indicate that the properties are
distributed across an oval with the center having a higher concentration of
properties.  `availability_365` is 0 in ~17,500 cases (~37% of observations).
As shown in the violin plot below, Brooklyn and Manhattan have the largest 
number of 0 availability properties and are skewed heavily towards having
less nights available. Bronx and Staten Island are more evenly distributed. We 
might infer that there are more home/apartment owners in Brooklyn and Manhattan
who are renting out their property for a small portion of the year.

```{r}
dataset %>%
  filter(availability_365 == 0) %>%
  nrow()
```

```{r}
dataset %>%
  ggviolin(x = "neighbourhood_group", y = "availability_365",
           trim = TRUE, xlab = "Neighbourhood Group", ylab = "Days Available")
```

Looking at the density of observations of `price` vs. the normal distribution drawn from the same mean and standard deviation, it is obvious they are quite different. The bottom left hand blot below shows the quantile-quantile (QQ) plot for `price` vs. normal distribution; if `price` was normally distributed, the line would be close to straight.  If we apply a log transformation to `price` and compare the result to the normal distribution, they are much more closely aligned.  Indeed, the QQ plot in the bottom right is much more linear.  Many statistical models assume that the response variable follows a normal distribution.  When we attempt to predict `price` further down, we will apply a log transformation so that our model predicts `log(price)`.  The true price prediction can then be extracted.

Note: `price` is sometimes 0, and $\log(0)$ is undefined.  To avoid this problem, we apply the transformation $\log(price + 1)$.

```{r}
mean <- mean(dataset$price)
sd <- sd(dataset$price)
normal_sample <- rnorm(1000, mean = mean, sd = sd)

ggplot(dataset, aes(x = price)) +
  stat_function(fun = dnorm, n = 100, args = list(mean = mean, sd = sd), color = "red") +
  labs(title = "Price Density vs. Normal Distribution", ylab = "") +
  geom_density() +
  ggthemes::theme_tufte()

qqplot(normal_sample, dataset$price) +
  title("QQ Plot: Price vs. Normal")

mean_log <- mean(log(dataset$price + 1))
sd_log <- sd(log(dataset$price + 1))
normal_sample_log <- rnorm(1000, mean = mean_log, sd = sd_log)

ggplot(dataset, aes(x = log(price + 1))) +
  stat_function(fun = dnorm, n = 100, args = list(mean = mean_log, sd = sd_log), color = "red") +
  labs(title = "Log Price Density vs. Normal Distribution", ylab = "") +
  geom_density() +
  ggthemes::theme_tufte()

qqplot(normal_sample, log(dataset$price)) +
  title("QQ Plot: Log Price vs. Normal")

```

Is there a relationship between the number of nights a property is available and
its cost?  Properties that are available for a majority of the year are likely 
taken more seriously as business ventures (indeed they might be purely business
ventures rather than, say, an individual renting out their converted basement to 
make some extra money).  The scatter plots below show that for most of the 
neighborhood groups, price and availability do not appear to be related.  However,
Manhattan apartments tend to increase in price with their availability. 
```{r}
dataset %>%
  group_by(availability_365, neighbourhood_group) %>%
  summarise(average_price = mean(price, na.rm = TRUE)) %>%
  filter(average_price < 1000) %>%
  ggplot(aes(x = availability_365, y = average_price)) +
  geom_point() +
  facet_wrap(~neighbourhood_group)
```

The scatter plots above also indicate that Manhattan is the most expensive 
neighbourhood group to rent from, while Queens, Bronx, and Staten Island all look
similarly inexpensive.

To view the distribution of price by neighbourhood group, we'll plot another 
violin plot (below).  This plot has its y-axis (price) on a log10 scale because 
while there are a wide range of prices, most properties are priced less than 200.  
Without the log scale, it would be hard to see the distribution among properties 
in this price range.  The plot confirms that Manhattan is the most expensive area 
- only 25% of its properties are priced at less than $100/night, but Bronx, Queens,
and Staten Island all have close to 75% of their properties for less than 
$100/night.

```{r}
ggviolin(dataset, x = "neighbourhood_group", y = "price", 
         trim = TRUE, yscale = "log10", draw_quantiles = c(0.25, 0.5, 0.75))
```

To view the prices on a map, we'll use the leaflet package.  We'll bin the 
locations into a rectangular grid and calculate the average price within each
grid.  A continuous color palette will allow us to visualize the magnitude of 
the average price.  Note that we could also plot the individual locations, coloring
each point based on the price.  However, since there are 50,000 locations it would
be difficult to get a sense for the distribution.

We can see that the most expensive area to rent is Manhattan (confirming
what we already saw in the violin plots above).  However, we can now also see
that the most expensive regions to rent are located along the waterfronts!

```{r}
# create a data frame where each row is the (long1, long2, lat1, lat2)
# coordinates in a 50 x 50 grid covering the longitude and latitude range
# present in the data set.
long_part = 50
lat_part = 50

long_delt = (max(dataset$longitude) - min(dataset$longitude)) / long_part
lat_delt = (max(dataset$latitude) - min(dataset$latitude)) / lat_part

longs = seq(min(dataset$longitude), max(dataset$longitude), by = long_delt)
lats  = seq(min(dataset$latitude), max(dataset$latitude), by = lat_delt)

grid <- expand.grid(longs, lats)

names(grid) <- c("long1", "lat1")

grid <- grid %>%
  mutate(long2 = long1 + long_delt, lat2 = lat1 + lat_delt) 

to_plot <- filter(dataset, !is.na(longitude) & !is.na(latitude))

# conditionally join the grid to the original data set so that each location
# is identified with a square in the grid.  we could also do a cartesian join
# and then filter based on containment using dplyr, but this could potentially
# cause OOM issues on some machines --- there are 2,500 grid points and 50,000
# locations.
merged <- sqldf('SELECT
         long1, long2, lat1, lat2, longitude, latitude, id, price
       FROM
         grid
         LEFT JOIN dataset
         ON longitude >= long1
            AND longitude < long2
            AND latitude >= lat1
            AND latitude < lat2
       WHERE
         longitude IS NOT NULL
         AND latitude IS NOT NULL')

# get the average price per square and create a palette based on the (natural)
# log of those values
squares <- merged %>%
  group_by(long1, long2, lat1, lat2) %>%
  summarise(price = mean(price, na.rm = T), obs = n())

pal <- colorNumeric(
  palette = "YlGnBu",
  domain = range(log(squares$price))
)

myf <- function(x) { round(exp(1) ^ x, -2) }

leaflet(squares) %>%
  addTiles() %>%
  addRectangles(lng1 = ~long1, lat1 = ~lat1, lng2 = ~long2, lat2 = ~lat2,
                fillColor = ~pal(log(squares$price)),
                fillOpacity = 0.7,
                popup = ~price,
                weight = 1,
                smoothFactor = 0.2) %>%
  addLegend(pal = pal, values = ~log(price), title = "Price", labFormat = labelFormat(transform = myf))

```


## Prepare features for modeling.

We'll be predicting price as a function of most of the features in the original dataset. In particular,
`id`, `host_id`, and `host_name` will dropped since they are mostly distinct across the observations.
Categorical variables are one-hot encoded with full rank; that is, a categorical variable with N levels
will be encoded into N-1 columns so that no linear dependences are induced between columns. Finally,
price will be log-scaled to make its distribution closer to the uniform distribution.  As discussed
above, the transformation will be $price\rightarrow \log(price + 1)$ instead of 
$price\rightarrow\log(price)$ since `price` is sometimes 0 and $\log(0)$ is undefined.

Select relevant columns:
```{r}
to_model <- dataset %>%
  select(price, neighbourhood, latitude, longitude, room_type, neighbourhood_group, 
         minimum_nights, number_of_reviews, reviews_per_month,
         availability_365, calculated_host_listings_count, days_since_review) 
```

Log-transform price:
```{r}
to_model <- to_model %>%
  mutate(log_price = log(price + 1)) %>%
  select(-price)
```

# lasso fusion categorical https://stats.stackexchange.com/questions/146907/principled-way-of-collapsing-categorical-variables-with-many-levels

One-hot encoding:
```{r}
library(caret)

dmy <- dummyVars("~ .", data = to_model, fullRank = T)

to_model <- data.frame(predict(dmy, newdata = to_model))  
```


## Predictions via XGBoost.

We'll be leveraging H2O's implementation of XGBoost to predict a rental's price, using grid search to approximate the optimal model parameters.  

`id`, `host_id`, and `host_name` will dropped since they are mostly distinct across
the observations.  

Split into train and test
```{r}
set.seed(1234)

to_model <- to_model %>%
  mutate(id = row_number())

train <- to_model %>%
  sample_frac(0.8)

test <- anti_join(to_model, train, by = "id")
```

Set response and features
```{r}
response <- "log_price"
features <- setdiff(colnames(to_model), c(response, "id"))
```

Helper functions
```{r}
mse_unlog <- function(pred_log, true_log) {
  pred <- exp(pred_log)
  true <- exp(true_log)
  
  mean((pred - true) ^ 2)
}

rmse_unlog <- function(pred_log, true_log) {
  pred <- exp(pred_log)
  true <- exp(true_log)
  
  sqrt(mean((pred - true) ^ 2))
}
```


Baseline model MSE and RMSE and R^2 (= 1 - sum_squared_error / sum_error_mean_model)
```{r}
mean_log_price <- mean(train$log_price)
pred_log_price <- rep(mean_log_price, nrow(test))
true_log_price <- test$log_price

mse_unlog(pred_log_price, true_log_price)
rmse_unlog(pred_log_price, true_log_price)
```

H2O preprocessing
```{r}
library(h2o)

# start and connect to H2O instance; defaults to using all available threads
h2o.init()

# don't show progress bar during h2o processes
h2o.no_progress()

train_h2o <- as.h2o(train)
test_h2o  <- as.h2o(test)
```


Default XGB
```{r}
h2o_xgb <- h2o.xgboost(
  x = features,
  y = response,
  training_frame = train_h2o,
  nfolds = 5,
  ntrees = 50,
  learn_rate = 0.3,
  distribution = "gaussian"
)

h2o.performance(h2o_xgb, test_h2o)

h2o_pred <- h2o.predict(h2o_xgb, test_h2o) %>% as.vector()

rmse_unlog(h2o_pred, true_log_price)


```

Hyperparameter search
```{r}
hyper_parameters <- list(sample_rate = c(0.5, 0.75, 1),
                         col_sample_rate = c(0.6, 0.8, 1),
                         max_depth = c(5, 6, 7))

ntrees <- 100

grid <- h2o.grid("xgboost",
                 hyper_params = hyper_parameters,
                 x = features,
                 y = response,
                 distribution = "gaussian",
                 training_frame = train_h2o,
                 nfolds = 5,
                 ntrees = ntrees,
                 learn_rate = as.numeric(2 / ntrees)
)

grid_models <- lapply(grid@model_ids, h2o.getModel)

for (i in 1:length(grid_models)) {
  h2o_pred <- h2o.predict(grid_models[[i]], test_h2o)
  rmse <- rmse_unlog(h2o_pred, true_log_price)
  print(paste0("Model: ", i, " | RMSE: ", rmse))
}
```


```{r}
# catboost

```




